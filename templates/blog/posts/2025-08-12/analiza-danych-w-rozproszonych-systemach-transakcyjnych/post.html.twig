{% extends 'blog/post.html.twig' %}

{%- block title -%}
    {{ post.title }}
{%- endblock -%}

{%- block description -%}
    {{ post.description }}
{%- endblock -%}

{% block article %}
    <div class="img-wide">
        <img src="{{ asset('images/blog/analytics-in-transactional-distributed-systems/analytics_01.jpg') }}" alt="Analiza danych w rozproszonych systemach transakcyjnych" />
    </div>

    <h1 class="font-bold text-4xl mb-2" id="title">{{ post.title }}</h1>
    <div class="mb-2">
        <small class="text-sm">Data Publikacji {{ post.date | date }}</small>
    </div>
    <div class="mb-4">
        {% for label in post.labels %}
            <small><span class="badge badge-info">{{ label }}</span></small>
        {% endfor %}
    </div>
    <p>
        W tym wpisie postaram się poruszyć problem analizy danych w rozproszonych systemach transakcyjnych.<br/>
        Jeżeli szukasz pomysłów na zbudowanie centralnego magazynu danych, który pozwoli zebrać dane z całego systemu,
        niezależnie od jego fragmentacji a przy okazji nie utonąć w kosztach operacyjnych, to ten wpis jest dla Ciebie.
    </p>

    <h2>Wszystko zaczyna się niewinnie</h2>
    <p>
        Większość systemów, które na co dzień tworzymy, przechowuje dane w jakiejś relacyjnej bazie danych.
        Bardzo popularnym, a przy okazji dobrym wyborem jest PostgreSQL, który w ostatnich latach stał się niemalże
        standardem w branży.
    </p>
    <p>
        Historia większości projektów wygląda przeważnie bardzo podobnie: zaczynamy od weryfikacji pomysłu, zdobywamy
        pierwszych użytkowników, system zaczyna zarabiać, biznes kombinuje jak zwiększyć zyski, powstają nowe
        funkcjonalności. Każda nowa funkcjonalność to kilka nowych tabelek w bazie danych.
    </p>
    <p>
        W celu przyśpieszenia developmentu korzystamy z ORM-a, automatycznie generujemy migracje, które tworzą i
        aktualizują schemat bazy danych.
    </p>
    <p>
        Początkowo wszystko idzie gładko, nowe funkcjonalności przynoszą spodziewane zyski, biznes zaczyna się skalować.
        Zatrudniamy więcej programistów, aby tworzyć więcej funkcjonalności równolegle.
    </p>
    <p>
        Od czasu do czasu ktoś zgłasza, że system w niektórych miejscach zaczyna "zamulać", szybki rekonesans, jeszcze
        szybsza
        diagnoza, brakuje indeksu w jakiejś tabelce.
    </p>
    <p>
        W konfiguracji mapowań ORM-a dokładamy indeks na pole, po którym system bardzo często wyszukuje dane.
        Problem rozwiązany.
    </p>
    <p>
        Powiększający się zespół programistów przykłada dużą wagę do jakości, być może nawet posługuje się
        zaawansowanymi technikami wytwarzania oprogramowania, jak Event Storming czy Domain-Driven Design.<br/>
        CI/CD wykonuje niezliczone ilości testów, upewniając się, że zmiany nie wprowadzają regresji.
    </p>
    <p>
        Idylla trwa, zespół lub być może wiele zespołów zaczyna dokładać nowe moduły do systemu. Moduły odpowiednio
        odizolowane, odpowiedzialne za konkretne zadania, nigdy nie przekraczające swoich granic i nie wchodzące
        w kompetencje innych modułów.
    </p>
    <p>
        Do komunikacji wykorzystywane są oczywiście kolejki, implementujemy
        <a href="https://event-driven.io/en/outbox_inbox_patterns_and_delivery_guarantees_explained/" target="_blank">Outbox/Inbox
            Pattern</a>
    </p>
    <p>
        Aby zapewnić odpowiednią izolację, ustalamy reguły mówiące o tym, że każdy moduł ma dostęp jedynie
        do tych tabelek w bazie danych, które do niego należą. W celu uzyskania danych z innego modułu należy
        udać się do tego modułu, czy to za pośrednictwem jakiegoś wewnętrznego API, czy w jakikolwiek inny sposób.
    </p>
    <p>
        Co jakiś czas biznes przychodzi do nas z pytaniem <strong>czy możecie na szybko wygenerować dla nas ten
            raport?</strong>.
        Oczywiście, kilka linijek w SQL'u, być może kilkadziesiąt i raport gotowy.
    </p>
    <p>
        Biznes zadowolony, raport w postaci CSV leci do Excela (najbardziej popularnego narzędzia BI), biznes wyciąga
        wnioski,
        planuje nowe funkcjonalności i zmiany.
    </p>
    <div class="img-wide">
        <img src="{{ asset('images/blog/analytics-in-transactional-distributed-systems/happy_business_01.jpg') }}" alt="Business Intelligence" />
    </div>

    <h2>Czas płynie, nowe tabelki wyrastają jak grzyby po deszczu</h2>
    <p>
        W takim stanie rzeczy możemy trwać bardzo długo, nawet kilka dobrych lat.
    </p>
    <p>
        W międzyczasie ktoś gdzieś na pewno wpadnie na pomysł, żeby dodać do systemu możliwość generowania raportów.
        To tylko i wyłącznie kwestia czasu.
    </p>
    <p>
        Raporty o stanie systemu to dla biznesu jedno z bardziej kluczowych narzędzi dających wgląd w zachowania,
        preferencje
        czy trendy użytkowników. Pozwalają nie tylko zrozumieć, co się dzieje, ale też odpowiednio zaplanować to, co się
        ma dopiero wydarzyć.
    </p>
    <p>
        Im lepsze i bardziej szczegółowe raporty, tym lepsze decyzje można na ich podstawie podejmować. Dobre
        decyzje biznesowe przekładają się na większe zyski, większe zyski przekładają się na większy budżet.
        Większy budżet przekłada się na lepsze narzędzia, większe zespoły, lepsze wynagrodzenia czy premie.
    </p>
    <p>
        W interesie każdego programisty powinno być więc dostarczanie biznesowi możliwie jak najlepszych i jak
        najbardziej precyzyjnych danych, w końcu lepsze wyniki przekładają się bezpośrednio na lepsze zyski.
    </p>
    <h2>Pierwsze objawy</h2>
    <p>
        System działa, przynosi zyski. Składa się z około 5, może nawet 10 modułów, każdy moduł składa się z 20-50
        tabelek w
        bazie danych. Każdy moduł dostarcza swoje własne raporty.
    </p>
    <ul>
        <li>Sprzedaż</li>
        <li>Marketing</li>
        <li>Logistyka</li>
        <li>Stany Magazynowe</li>
        <li>Użytkownicy</li>
    </ul>
    <p>
        Każdy moduł udostępnia tylko część danych, cząstkę większego obrazu, żaden jednak nie daje podglądu na całość.
    </p>
    <p>
        Zespoły wprawdzie zaimplementowały klucze referencyjne do danych pochodzących z innych modułów, udało się nawet
        w interfejsie użytkownika stworzyć jedno miejsce z którego można wygenerować raporty.
    </p>
    <p>
        To jednak dalej za mało...
    </p>
    <p>
        Bardzo szybko okazuje się, że raporty generowane w różnych modułach, napisane przez różnych programistów, być
        może nawet w różnych technologiach mają różne formaty danych, różne standardy nazewnictwa.
    </p>
    <p>
        Inaczej interpretowane są zakresy dat, jeden moduł uwzględnia daty końca i początku, drugi je wyklucza,
        a jeszcze inny robi przedział prawostronnie otwarty w celu ułatwienia paginacji, bo akurat mają
        też API i to API wykorzystuje ten sam kawałek kodu.
    </p>
    <p>
        Ponieważ każdy moduł jest niezależny, posiada swoje granice, swóją nomenklaturę, w pewnym momencie orientujemy
        się,
        że to, co w jednym module nazywamy w jakiś sposób, inny moduł eksponuje pod zupełnie inną nazwą.
        Ponieważ w kontekście tego modułu ma to sens.
    </p>
    <p>
        Po czasie pewnie też się zorientujemy, że każdy zespół inaczej zdefiniował sobie politykę retencji i
        przechowywania danych.
        Pomimo posiadania w kluczowym module danych z ostatnich 5 lat, nie możemy nic z nimi zrobić, bo moduły, które
        dostarczały
        dane potrzebne do wzbogacenia podstawowego raportu, posiadają dane jedynie z ostatnich 2 lat.
    </p>
    <p>
        Nie są to jednak problemy, których odrobina magii w Excelu nie byłaby w stanie rozwiązać (być może poza brakami
        w danych).
        Tym kolumnom zmienimy nazwy, te usuniemy, dodamy szybkie filtrowanko i wystarczy.
    </p>
    <p>
        Stworzymy sobie jeden wielki plik, w którym będziemy mieli jeden arkusz o nazwie "Dashboard", a wszystkie
        inne będą tylko do odczytu, będą zasilały dashboard.
    </p>
    <p>
        Być może to podejście będzie nawet chwilę działać. Być może nawet dłużej niż chwilę, ale nie miejmy złudzeń.
        To wszystko w końcu padnie, i to zgodnie z prawami
        <a href="https://en.wikipedia.org/wiki/Murphy%27s_law" target="_blank">Murphiego</a>
        padnie w najgorszym możliwym momencie.
    </p>
    <h2>Co złego jest w Excelu?</h2>
    <p>
        Nic! Excel to rewelacyjne narzędzie. Problem nie leży w Excelu, ale w jego wykorzystaniu.
    </p>
    <p>
        Ta cała magia polegająca na oczyszczeniu i przygotowaniu danych nie powinna mieć miejsca w Excelu, nie
        na większą skalę. Jeżeli mówimy o jednorazowym szybkim raporcie, nie ma problemu. Robimy co musimy,
        klepiemy formułki, analizujemy dane i zapominamy.
    </p>
    <p>
        Jeżeli jednak ma to być część naszej codziennej rutyny, jeżeli cyklicznie musimy przechodzić przez ten sam
        proces, podążając za nieustannymi zmianami i ewolucją systemu, prędzej czy później okaże się, że te arkusze są
        nieaktualne.
    </p>
    <p>
        Kolumny przestały istnieć lub zmieniły nazwy, powstały nowe kolumny, format danych uległ zmianie albo
        co gorsza, jeden z zespołów opiekujący się jednym z modułów usunął jakieś dane bez świadomości, że były one
        wykorzystywane
        przez jakiegoś użytkownika biznesowego gdzieś w jednym z jego raportów, które otwiera raz na kwartał.
    </p>
    <p>
        Na dłuższą metę, bardziej złożone arkusze kalkulacyjne, które czerpią dane z automatycznie generowanych przez
        system raportów, które są następnie sklejane na podstawie niejawnych reguł, są nie do utrzymania.
    </p>
    <h2>To może podepniemy jakieś narzędzie BI?</h2>
    <p>
        Pomyślało wielu programistów, którzy wielokrotnie zetknęli się z problemem generowania raportów.
    </p>
    <p>
        Weźmy na przykład taki <a href="https://www.metabase.com/" target="_blank">Metabase</a>. Darmowe narzędzie,
        które
        możemy postawić w kilka minut za pomocą Dockera.
    </p>
    <p>
        Dać mu dostęp do naszej bazy i kilku lub wszystkich tabelek, i za pomocą bardzo przyjaznego interfejsu
        użytkownika biznes będzie mógł w bardzo łatwy i przyjemny sposób wygenerować najbardziej skomplikowane raporty.
    </p>
    <p>
        Raporty, które będą mogły zawierać dane z wielu modułów jednocześnie!
    </p>
    <p>
        Możemy nawet zatrudnić analityka danych z podstawami SQL'a, który wszystko to, czego nie będzie
        się dało wyklikać, osiągnie za pomocą odpowiednio przygotowanego zapytania.
    </p>
    <h2>Tylko, że to nie rozwiązuje problemu</h2>
    <p>
        Jedynie odsuwa go w czasie.
    </p>
    <p>
        Jeżeli przyjrzymy się dokładnie, co się zmieniło, to zmieniła się tylko jedna rzecz. Narzędzie...
        Przenieśliśmy problem oczyszczania i łączenia danych z Excela do Metabase.
    </p>
    <p>
        Excel wprawdzie wrócił do swojej pierwotnej roli, możemy teraz raporty pobrane z Metabase wrzucić do Excela.
    </p>
    <p>
        Jednak nasza niejawna logika łączenia/oczyszczania danych przeniosła się z arkusza kalkulacyjnego do zapytań
        SQL.
    </p>
    <p>
        Poza tym, wszystkie problemy zostały takie same:
    </p>
    <ul>
        <li>niespójność danych</li>
        <li>niespójność nazewnictwa</li>
        <li>brak jednolitej polityki wstecznej kompatybilności</li>
        <li>brak jednolitej polityki retencji danych</li>
    </ul>
    <h2>To może ustanowimy procesy i reguły?</h2>
    <p>
        Większość z powyższych problemów da się rozwiązać implementując odpowiednie procesy oraz reguły.
    </p>
    <p>
        Możemy ustalić standardy nazewnictwa mówiące, że każda tabelka w bazie musi zawierać w nazwie prefix modułu, a
        kolumny nazywane są małymi literami i odseparowane podkreśleniami.
    </p>
    <p>
        Możemy ustalić, że każdy moduł przechowuje dane z ostatnich 5 lat (hot storage), wszystko co starsze jest
        archiwizowane. (cold storage)
    </p>
    <p>
        Możemy ustalić, że zakresy dat zawsze traktowane są jako przedziały prawostronnie otwarte.
    </p>
    <p>
        Możemy ustalić, że nie usuwamy żadnych kolumn z bazy danych, albo że przed usunięciem czegokolwiek najpierw
        wchodzimy w okres przejściowy, podczas którego pokazujemy każdemu użytkownikowi systemu,
        które kolumny zmienią się i w jaki sposób.
    </p>
    <p>
        Nawet jeżeli przyjmiemy na potrzeby dyskusji, że uda się te procesy globalnie zaimplementować pomiędzy kilkoma
        zespołami,
        oraz że te zespoły będą ich bezwzględnie i bardzo dokładnie przestrzegać, <strong>to nie wystarczy...</strong>
    </p>
    <h2>Skalowanie bazy danych nie jest tanie</h2>
    <p>
        Szczególnie jeżeli opieramy się o rozwiązania chmurowe.
    </p>
    <p>
        Wyobraźmy sobie sytuację, w której w szczytowych godzinach pracy systemu (kiedy użytkownicy generują najwięcej
        transakcji)
        analityk biznesowy, który pracuje według własnego planu musi wygenerować raport w oparciu o taki typowy, SQLowy
        kilkutysięcznik?
    </p>
    <p>
        Analityk odpala zapytanie, baza danych zaczyna mielić. Zapytanie trwa, 5, 10, 15 minut.
        Baza danych zaczyna się pocić.
    </p>
    <p>
        Użytkownicy bombardują system nowymi zamówieniami (lub jakimikolwiek innymi operacjami, które generują sporo
        zapisów)
        w czasie kiedy analityk czeka na wyniki.
    </p>
    <p>
        W tym samym momencie ktoś z biznesu potrzebuje na szybko sprawdzić kilka raportów, każdy z nich zawiera
        "całkowitą liczbę wierszy w tabelce".
        Takich osób jest kilka.
    </p>
    <p>
        Wszystkie te operacje nakładają się na siebie, nasza już i tak bardzo obciążona baza danych nie wyrabia.
    </p>
    <p>
        Niektóre transakcje użytkowników nie dochodzą do skutku. <br/>
        System ledwo dyszy. Czas oczekiwania na najbardziej podstawowe operacje mierzony jest w sekundach.
    </p>
    <p>
        A teraz wisienka na torcie, kiedy te wszystkie dantejskie sceny mają miejsce, kiedy Pager Duty jest rozgrzany do
        czerwoności od wszelkiego typu i rodzaju incydentów, kiedy zespoły w panice próbują przywrócić system do życia,
        devopsi kombinują jak na szybko przeskalować bazę danych...
    </p>
    <div class="img-wide">
        <img class="mt-[-150px]" src="{{ asset('images/blog/analytics-in-transactional-distributed-systems/construction_01.jpg') }}" alt="Prace remontowe" />
    </div>
    <p>
        CEO zaczyna prezentację dla potencjalnego partnera biznesowego, z którym współpraca
        ma okazać się kluczowa w strategii rozwoju firmy...
    </p>
    <h2>To może po prostu postawmy replikę?</h2>
    <p>
        W końcu raporty nie będą przeciążać naszej bazy transakcyjnej.
    </p>
    <p>
        Podwoimy wprawdzie koszty utrzymania bazy danych, ale zredukujemy ryzyko przeciążenia systemu i będziemy
        mogli podpiąć ulubione narzędzie business intelligence wprost na replikę, co da nam dane real time.
    </p>
    <p>
        Brzmi rewelacyjnie, ale w praktyce nie jest to tak proste.
    </p>
    <p>
        Pomijając już nawet potencjalne problemy wynikające z samej natury replikacji, głównym i podstawowym problemem
        na który najczęściej trafiam, jest <strong>percepcja</strong>.
    </p>
    <p>
        Zupełnie inaczej na tabele w bazie danych będzie patrzył programista, który te tabele wygenerował za pomocą
        mapowań ORM-a, niż analityk danych.
    </p>
    <p>
        Programista będzie wiedział, które tabele należy połączyć razem, aby otrzymać obraz całości.
        Będzie rozumiał ograniczenia i warunki zaszyte gdzieś w kodzie aplikacji.
        Przede wszystkim programista zna lub chociaż powinien się orientować jak wygląda cykl życia systemu (jego
        danych).
    </p>
    <p>
        Ta cała wiedza najczęściej nie jest dostępna dla analityków.
    </p>
    <p>
        To tak jakby powiedzieć komuś, żeby spojrzał na coś przez dziurkę od klucza. Coś na pewno da się zobaczyć.
        Jakieś wnioski da się wyciągnąć, ale bardzo ciężko będzie odbudować całość.
    </p>
    <p>
        Wystarczy, że mamy w bazie danych kolumnę typu JSONB w której przechowujemy jakieś struktury danych.
        Załóżmy, że system dopuszcza 3 poprawne kombinacje tej samej struktury, ale jedna jest super rzadka, na tyle
        rzadka, że jeszcze w systemie nie wystąpiła. Patrząc na dane, nawet całościowo, analityk po prostu nie może wiedzieć,
        że istnieją 3 kombinacje jednej struktury. Podczas normalizacji uwzględni 2 przypadki, podczas gdy trzeci
        stanie się tykającą bombą zegarową, która wybuchnie jak zawsze w najmniej oczekiwanym momencie.
    </p>
    <p>
        Inaczej mówiąc, jeżeli mamy w systemie kilka niezależnych modułów. Każdy z swoją bazą danych, albo przynajmniej
        swoimi tabelami w bazie. Co sumarycznie daje nam 200-300 tabelek, oczekiwanie, że analityk to bez problemu
        ogarnie, nie popełni błędów i raporty nie będą odbiegały od oczekiwań, jest delikatnie mówiąc naiwne.
    </p>
    <p>
        Mimo wszystko wystawienie kopii/repliki bazy danych dla analityków i nadanie jej 4-literowej nazwy pochodzącej
        od słowa "analytics" dalej jest powszechnie stosowane.
    </p>
    <p>
        Narzędzia BI prześcigają się w tym, kto stworzy lepszy interfejs użytkownika, dzięki któremu raporty da się
        wyklikać.
        Obiecują, że będziemy mogli analizować dane bez SQL.
    </p>
    <p>
        Tak, to może działać, w wielu miejscach właśnie tak to działa. O czym jednak głośno nie mówimy to:
    </p>
    <ul>
        <li>Problemy z wsteczną kompatybilnością oraz zmianami struktury danych</li>
        <li>Problemy z odpowiednim utrzymaniem / wersjonowaniem / testowaniem gigantycznych zapytań SQL/skryptów
            normalizujących dane w locie
        </li>
        <li>Repliki/Kopie generują dodatkowe koszty</li>
        <li>Redukcja zasobów replik jest albo niemożliwa, albo uniemożliwia generowanie raportów w akceptowalnych
            czasach
        </li>
    </ul>
    <p>
        Co w rezultacie odbija się na jakości danych i skuteczności podejmowania decyzji biznesowych.
    </p>
    <h2>Co nam pozostaje?</h2>
    <p>
        Może najpierw ustalmy jakie problemy w pierwszej kolejności chcemy rozwiązać:
    </p>
    <div class="img-wide">
        <img src="{{ asset('images/blog/analytics-in-transactional-distributed-systems/strategy_01.jpg') }}" alt="Strategia i analiza" />
    </div>
    <ol>
        <li>Analizowanie danych / generowanie raportów nie może mieć żadnego wpływu na pracę systemu.</li>
        <li>Dane w raportach muszą być zawsze świeże (dopuszczalny jest opóźnienie w danych, ustalane indywidualnie)</li>
        <li>Raporty muszą odzwierciedlać realny, niewypaczony stan systemu</li>
        <li>Struktura danych musi być odporna na regresję</li>
        <li>Spójna polityka retencji i archiwizowania danych</li>
    </ol>
    <h2>1) Separacja Zasobów</h2>
    <p>
        Nie jest to nic odkrywczego, jeżeli nie chcemy, żeby nasz system był narażony na przeciążenia
        wynikające z nadużywania bazy danych poprzez generowanie raportów, musimy postawić sobie osobną bazę danych.
    </p>
    <p><strong>Jaką bazę wybrać pod analitykę?</strong></p>
    <p>
        To jest w zasadzie temat na osobny artykuł albo nawet serię artykułów.
        Rozwiązań jest bardzo dużo, jedne lepsze, inne gorsze. Nie istnieje jedno
        magiczne rozwiązanie na wszystkie problemy.
    </p>
    <p>
        Moja rada, szczególnie dla mniejszych zespołów, niedoświadczonych w zarządzaniu danymi jest taka, żeby nie
        rzucać się na technologie, z którą nie mamy doświadczenia.
    </p>
    <p>
        Kluczowy jest odpowiedni format danych. Po zamianie wielu wąskich tabelek na jedną szeroką najprawdopodobniej
        okaże się, że generowanie tego samego raportu tylko bez używania 20x <code>JOIN</code> nie zajmuje już 10 minut
        a mniej niż pół sekundy.
    </p>
    <p>
        A co jeżeli problemem są agregacje, a nie łączenia?
    </p>
    <p>
        Wtedy, zamiast agregować w locie, lepiej przygotować tabelę zawierającą te dane w formie zagregowanej, a nie
        surowej.
    </p>
    <h2>2) Świeże Dane</h2>
    <p>
        No dobra, ale skoro tworzymy nową, niezależną bazę danych, to w jaki sposób zadbamy o to aby dane w tej
        bazie były świeże i aktualne?
    </p>
    <p>
        Tutaj bardzo dużo zależy od dopuszczalnego opóźnienia w synchronizacji danych.
        Najczęściej wystarczy jak baza analityczna jest około 24 godziny za bazą transakcyjną. Czyli zawiera
        dane do "wczoraj", uwzględniając całe "wczoraj".
    </p>
    <p>
        Dlaczego? Bo mało które decyzje biznesowe podejmowane są w danej chwili.
        Jeżeli jakieś decyzje muszą być podejmowane w tak krótkim czasie, wtedy buduje się właściwe automatyzacje.
    </p>
    <p>
        Jeżeli 24-godzinne opóźnienie jest akceptowalne (czasami nie jest i na to też są sposoby),
        wystarczy, że synchronizacje przeprowadzimy kilka razy dziennie.
        Oczywiście tu też nie ma złotej reguły. Tak samo jak nie ma reguły mówiącej jak duży zakres synchronizować na raz.
    </p>
    <p>
        Jest za to jedna dobra praktyka, która ułatwia synchronizację. Polega ona na upewnieniu się, że główne tabele w
        systemie transakcyjnym zawierają datę utworzenia/modyfikacji rekordu.
    </p>
    <p>
        Posiadając te dwie informacje jesteśmy w stanie zawęzić okno synchronizacji do jakiegoś określonego okresu czasu.
    </p>
    <p>
        Jak to w praktyce wygląda? Możemy np. odpalać proces synchronizacji co 6 godzin, zbierając tylko rekordy zmienione w
        ciągu ostatnich 24 godzin.<br/>
        <code>Oczywiście to są przykładowe liczby, te wartości trzeba ustalić na podstawie rozmiaru i zachowania danych.</code>
    </p>
    <p>
        Dlaczego z 24 godzin? Takie dodatkowe zabezpieczenie. Moglibyśmy pobierać dane tylko z 7 godzin, ale jeżeli z jakiegokolwiek
        powodu synchronizacja się nie wykona, a my tego nie wyłapiemy, możemy stracić dane.
    </p>
    <h2>3) Odzwierciedlenie Stanu Systemu</h2>
    <p>
        Moja opinia na ten temat może wydać się kontrowersyjna, ale uważam, że najlepszą wiedzę na temat danych i zachowania
        systemu czy modułu ma zespół, który ten system/moduł buduje.
    </p>
    <p>
        To właśnie ten zespół powinien być odpowiedzialny za to, żeby dane, które są generowane przez system lub jego część
        za którą dany zespół odpowiada, trafiały do centralnego repozytorium danych.
    </p>
    <p>
        Innymi słowy, to właśnie zespół implementujący daną funkcjonalność powinien na podstawie zebranych wcześniej wymagań
        przekształcić te dane do odpowiedniego formatu i wypchnąć dalej.
    </p>
    <p>
        Jest to chyba najłatwiejszy sposób upewnienia się, że dane są kompletne a programiści z danego zespołu są
        świadomi, że te dane są gdzieś wykorzystywane. Format danych analitycznych staje się dla nich
        swego rodzaju kontraktem – kontraktem, którego muszą przestrzegać.
    </p>
    <p>
        Nie różni się to bardzo od kontraktu na schemat API.
    </p>
    <h2>4) Odporność na regresję</h2>
    <p>
        Ten punkt jest chyba najbardziej skomplikowany. Poprawna implementacja ewolucji schematu danych jest
        często nie tyle trudna, co kłopotliwa.
    </p>
    <p>
        W dużym skrócie zasady wyglądają tak:
    </p>
    <ul>
        <li>Nigdy nie usuwamy kolumn</li>
        <li>Wszystkie kolumny, które dodajemy muszą być <code>nullable</code> albo posiadać wartość domyślną</li>
        <li>Typy kolumn możemy tylko rozszerzać przykładowo, <code>int</code> możemy zamienić na <code>bigint</code> ale nie na odwrót</li>
        <li>Nie zmieniamy nazw kolumn</li>
    </ul>
    <p>
        Czy w takim razie nie możemy niczego usuwać?
    </p>
    <p>
        Możemy, ale nie byle jak. Generalnie to jak i jak często będziemy łamać wstęczną kompatybilność zależy tylko od nas.
    </p>
    <p>
        Jeżeli z naszego źródła danych analitycznych korzystamy tylko wewnętrznie i, powiedzmy, analityk zajmujący się budowaniem
        raportów jest na bieżąco z zmianami w systemie, przy odpowiedniej koordynacji moglibyśmy dodać
        nowe tabelki, a następnie usunąć stare, dając mu chwilę na zaktualizowanie raportów.
    </p>
    <p>
        Jeżeli jednak nasze źródło danych analitycznych wykorzystywane jest do <code>Data Science</code>, ale pracujemy w środowisku
        multi-tenancy i dane analityczne/raporty udostępniane są klientom, wtedy do sprawy musimy podejść zupełnie inaczej.
    </p>
    <h2>Polityka przechowywania i archiwizowania danych</h2>
    <p>
        Tak jak wspominałem wyżej, bardzo istotne jest, aby dane w bazie analitycznej, szczególnie dostarczane przez różne
        moduły podlegały tym samym regułom odnośnie czasu przechowywania.
    </p>
    <p>
        Jeżeli stany magazynowe w systemie trzymamy tylko z ostatniego roku, a zamówienia z ostatnich 5 lat,
        Analitycy nie będą w stanie zbudować raportu, który będzie zawierał dane z obu tych źródeł.
    </p>
    <p>
        Jest to bardziej problem natury formalnej niż technicznej. Wydawało by się, że wystarczy się po prostu dogadać,
        jednak w praktyce nie jest to takie proste.
    </p>
    <p>
        Aby ustalić wspólną politykę przechowywania i archiwizowania danych należy wziąć pod uwagę nie tylko aspekty
        techniczne, ale również prawne, biznesowe czy właśnie analityczne, co może wymagać kompromisów.
    </p>
    <h2>Przykłady</h2>
    <p>
        Popatrzmy teraz na prosty przykład procesu ETL, którego zadaniem jest przeniesienie danych z bazy transakcyjnej
        do bazy analitycznej.
    </p>
    <blockquote>
        W tym przykładzie wykorzystam <a href="https://flow-php.com" target="_blank">Flow PHP</a>, nie jest to
        jednak coś specjalnie unikalnego dla PHP. W każdym języku możemy zbudować coś bardzo podobnego za pomocą
        jakiejkolwiek biblioteki ułatwiającej tworzenie aplikacji CLI oraz jakiegoś narzędzia do przetwarzania danych.
    </blockquote>
    <p>
        Poniższy przykład (w nieco zmienionej postaci) pochodzi z sesji live stream, którą miałem przyjemność nagrywać z Rolandem prowadzącym kanał <a href="https://nevercodealone.de/de" target="_blank">Never Code Alone</a>.
        Materiał video znajdziesz na portalu YouTube pod frazą "Flow PHP"
    </p>
    <p>
        Załóżmy, że tak mniej więcej wygląda format zamówień:
    </p>
    <pre><code class="code-shell" {{ stimulus_controller('syntax_highlight') }}>{{ source(template_folder ~ '/code-examples/schema.txt') | e('html') }}</code></pre>

    <p>
        Naszym celem jest przeniesienie tych zamówień do analitycznej bazy danych, przygotujmy więc schemat danych
        wejściowych jak i docelowych.
    </p>

    <pre><code class="code-php" {{ stimulus_controller('syntax_highlight') }}>{{ source(template_folder ~ '/code-examples/orders.php') | e('html') }}</code></pre>

    <p>
        Zwróćmy uwagę na to, że docelowa struktura tabeli nie jest już zorientowana na zamówienia, a na zamówione przedmioty.
        Naszym celem jest rozpakowanie przedmiotów zamówień tak, aby każdy był osobnym wierszem.
    </p>
    <p>
        Dzięki temu analityk, który będzie musiał wygenerować raport, nie będzie już musiał kombinować i rozpakowywać
        jsona w locie.
    </p>
    <p>
        Kolumna Adres też została rozbita na kilka kolumn, dzięki czemu raport będzie można łatwiej
        filtrować.
    </p>
    <p>
        Kolejną istotną transformacją jest zamiana <code>price</code> z <code>float</code> na <code>int</code>
        poprzez przemnożenie wartości zmiennoprzecinkowej przez 100.
    </p>
    <p>
        Ostatnią już zmianą będzie dodanie informacji o tym, w jakiej walucie podawane są ceny. Tylko skąd ta informacja
        pochodzi? Jest to właśnie bardzo istotny szczegół wynikający z niezbyt dobrej implementacji.
        W tym konkretnym przypadku wszystkie zamówienia są w dolarach. System o tym wie, programiści o tym wiedzą,
        ale osoba patrząca na tabelki w bazie bez kontekstu nie musi wcale posiadać takiej wiedzy.
    </p>
    <p>
        Nasza docelowa struktura powinna wyglądać mniej więcej w taki sposób:
    </p>

    <pre><code class="code-shell" {{ stimulus_controller('syntax_highlight') }}>{{ source(template_folder ~ '/code-examples/final-schema.txt') | e('html') }}</code></pre>

    <p>
        Następnym krokiem będzie utworzenie odpowiedniej tabelki w bazie analitycznej. Możemy to osiągnąć stosunkowo
        łatwo dzięki adapterowi dla Doctrine DBAL.
    </p>

    <pre><code class="code-php" {{ stimulus_controller('syntax_highlight') }}>{{ source(template_folder ~ '/code-examples/schema-provider.php') | e('html') }}</code></pre>

    <p>
        W bazie analitycznej przechowywać będziemy więc "uproszczoną" lub "znormalizowaną" wersję tabeli z zamówieniami.
        Normalizacja polega na rozpakowaniu przedmiotów zamówienia i uczynieniach ich osobnymi wierszami oraz
        rozbiciu kolumny "Adres" na kilka kolumn.
    </p>

    <p>
        Przyjrzyjmy się więc komendzie CLI, która będzie odpowiedzialna za przeniesienie danych z bazy transakcyjnej
        do bazy analitycznej.
    </p>

    <pre><code class="code-php" {{ stimulus_controller('syntax_highlight') }}>{{ source(template_folder ~ '/code-examples/order-import-command.php') | e('html') }}</code></pre>

    <blockquote>
        Oczywiście nie jest to najpiękniejsza ani nawet najbardziej poprawna forma. Normalnie komenda CLI nie zawierałaby
        definicji <code>pipeline'u ETL</code>, jednak na potrzeby przykładu jest to dobry start.
    </blockquote>

    <p>
        Dedykowany centralny magazyn danych, to niewątpliwie kusząca opcja, szczególnie w miejscach
        gdzie brak widoczności uniemożliwia sprawne podejmowanie decyzji.
    </p>
    <p>
        Na szczęscie jest to ten rodzaj funkcjonalności, którą można dołozyć w zasadzie na każdym etapie życia projektu.
    </p>
    <p>
        Może wymagać wprowadzenia dodatkowych procesów oraz pewnej dyscypliny od zespołów, jednak korzyści płynące z takiego rozwiązania są ogromne.
    </p>
    <ul>
        <li>Nie ma obawy o to, że analityka wpłynie na pracę systemu</li>
        <li>Mamy dostęp do wszystkich zakamarków naszego systemu, każdego mikroserwisu czy modułu</li>
        <li>Taka centralna baza danych to najlepszy prezent dla analityków</li>
        <li>Data Science nie polega już na przepalaniu czasu na oczyszczanie danych</li>
        <li>Możemy łatwo i bezpiecznie podpiąć w zasadzie dowolne narzędzia typu Business Intelligence</li>
        <li>Tworzymy kulturę pracy z danymi w ramach naszej organizacji</li>
    </ul>
    <p>
        Oczywiście jak wszystko co nowe, takie zmiany mogą wyglądać na trudne do wprowadzenia.
        Brak doświadczenia w pracy z danymi przynajmniej na początkowych etapach sprawia, że zadanie to może wydawać
        się wręcz niewykonalne.
    </p>
    <p>
        Z mojego doświadczenia wynika jednak, że najtrudniej jest zacząć, kiedy mamy już:
    </p>
    <ul>
        <li>Kilka pierwszych <code>Pipeline'ów</code> przetwarzających dane</li>
        <li>Kilka lub kilkanaście schematów naszych danych</li>
        <li>Jakieś bardziej złożone transformacje</li>
        <li>Przygotowane testy</li>
        <li>Ustalone procesy i procedury</li>
    </ul>
    <p>
        Praca idzie wręcz maszynowo.
    </p>
    <p>
        Warto jednak pamiętać, że nie ma jednego uniwersalnego rozwiązania, które będzie pasować do każdego systemu.
        W każdym przypadku należy dostosować podejście do specyfiki danego systemu i organizacji.
    </p>

    <h2>Jak zacząć?</h2>
    <p>
        Jeśli potrzebujesz pomocy w zakresie budowy centralnego magazynu danych, chętnie Ci pomogę.<br/>
        <a href="{{ url('consulting') }}">Skontaktuj się ze mną</a>, a wspólnie stworzymy rozwiązanie, które będzie idealnie dopasowane do Twoich potrzeb.
    </p>
    <p>
        Zachęcam również do odwiedzenia serwera <a href="https://discord.gg/5dNXfQyACW" target="_blank">Discord - Flow PHP</a>, na którym
        możemy porozmawiać bezpośrednio.
    </p>
    <div class="img-wide">
        <img src="{{ asset('images/blog/analytics-in-transactional-distributed-systems/consulting_01.jpg') }}" alt="Konsultacje" />
    </div>
{% endblock %}