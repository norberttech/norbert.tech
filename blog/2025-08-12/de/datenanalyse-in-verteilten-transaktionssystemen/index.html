<!doctype html>
<html lang="de">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <title>Datenanalyse in verteilten Transaktionssystemen</title>
    <meta property="og:title" content="Datenanalyse in verteilten Transaktionssystemen" />
    <meta name="twitter:title" content="Datenanalyse in verteilten Transaktionssystemen" >

    <meta name="description" content="Bricht Ihr Transaktionssystem unter der Last von Berichten zusammen? Suchen Sie nach einer MÃ¶glichkeit, eine einheitliche Datenquelle fÃ¼r Ihr verteiltes System zu schaffen? Erfahren Sie, wie Sie ein effizientes analytisches Data Warehouse aufbauen und hÃ¤ufige Fallstricke vermeiden.">
    <meta property="og:description" content="Bricht Ihr Transaktionssystem unter der Last von Berichten zusammen? Suchen Sie nach einer MÃ¶glichkeit, eine einheitliche Datenquelle fÃ¼r Ihr verteiltes System zu schaffen? Erfahren Sie, wie Sie ein effizientes analytisches Data Warehouse aufbauen und hÃ¤ufige Fallstricke vermeiden.">
    <meta name="twitter:description" content="Bricht Ihr Transaktionssystem unter der Last von Berichten zusammen? Suchen Sie nach einer MÃ¶glichkeit, eine einheitliche Datenquelle fÃ¼r Ihr verteiltes System zu schaffen? Erfahren Sie, wie Sie ein effizientes analytisches Data Warehouse aufbauen und hÃ¤ufige Fallstricke vermeiden.">

    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://norbert.wip/blog/2025-08-12/de/datenanalyse-in-verteilten-transaktionssystemen" />
    <meta property="og:image" content="https://norbert.wip/assets/images/avatar-8f3c52c37f20d07c5e1631e1512bdeca.jpeg" />
    <meta property="og:image:type" content="image/svg+xml" />
    <meta property="og:image:alt" content="Norbert Orzechowicz - Personal Website" />

    <meta name="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="https://norbert.wip/blog/2025-08-12/de/datenanalyse-in-verteilten-transaktionssystemen" />
    <meta name="twitter:image" content="https://norbert.wip/assets/images/avatar-8f3c52c37f20d07c5e1631e1512bdeca.jpeg">
    <meta name="twitter:site" content="@norbert_tech" />
    <meta name="twitter:creator" content="@norbert_tech" />

    <link rel="apple-touch-icon" sizes="180x180" href="https://norbert.wip/assets/images/favicons/apple-touch-icon-9cae7ee880b4fe0bd755d300e1bca71e.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://norbert.wip/assets/images/favicons/favicon-32x32-b7a4ad4b584ab95534144e071f0e8587.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://norbert.wip/assets/images/favicons/favicon-16x16-154ca21abc06ae116c8d7ffc5713c000.png">
    <link rel="shortcut icon" href="https://norbert.wip/assets/images/favicons/favicon-db409885df78dea389e6d0b036da382c.ico">

            <style>
            @import url('https://fonts.googleapis.com/css2?family=Cabin:ital,wght@0,400..700;1,400..700&display=swap');
        </style>
        <link rel="stylesheet" href="https://norbert.wip/assets/styles/app-61740c0110d91265dcc865cf927f89cb.css">
    
            
<script type="importmap">
{
    "imports": {
        "app": "https://norbert.wip/assets/app-930adf3462cf9ab60908eb1b74cf7ca7.js",
        "@oddbird/popover-polyfill": "https://norbert.wip/assets/vendor/@oddbird/popover-polyfill/popover-polyfill.index-8f1adf02d2fe31ba0e133b65d1faeece.js",
        "https://norbert.wip/assets/bootstrap.js": "https://norbert.wip/assets/bootstrap-d78d7e12c819dedf89372fb4824c072d.js",
        "htmx.org": "https://norbert.wip/assets/vendor/htmx.org/htmx.org.index-9c972342e01d7cc4830a1976259158c7.js",
        "iconify-icon": "https://norbert.wip/assets/vendor/iconify-icon/iconify-icon.index-4471e3c6a0bda14753f1327106be2b63.js",
        "@symfony/stimulus-bundle": "https://norbert.wip/assets/@symfony/stimulus-bundle/loader-9311b8ea36bad0f6168e687b4d6dee73.js",
        "@hotwired/stimulus": "https://norbert.wip/assets/vendor/@hotwired/stimulus/stimulus.index-6f73d85e0fa2bcb3802562288e3f3042.js",
        "https://norbert.wip/assets/@symfony/stimulus-bundle/controllers.js": "https://norbert.wip/assets/@symfony/stimulus-bundle/controllers-30e149621baab8220e8ae00cc40a4a23.js",
        "https://norbert.wip/assets/controllers/hello_controller.js": "https://norbert.wip/assets/controllers/hello_controller-55882fcad241d2bea50276ea485583bc.js",
        "https://norbert.wip/assets/controllers/clipboard_controller.js": "https://norbert.wip/assets/controllers/clipboard_controller-6aefa8a9dec3271dae2f05b464bf9204.js",
        "https://norbert.wip/assets/controllers/syntax_highlight_controller.js": "https://norbert.wip/assets/controllers/syntax_highlight_controller-ae10e4cee8b4dedbf232536d05654062.js",
        "highlight.js/lib/core": "https://norbert.wip/assets/vendor/highlight.js/lib/core-a4e70e8d9019e717dfd018057650e566.js",
        "highlight.js/lib/languages/php": "https://norbert.wip/assets/vendor/highlight.js/lib/languages/php-3082801e8bbfa191b25e190b817c8690.js",
        "highlight.js/styles/github-dark.min.css": "data:application/javascript,document.head.appendChild%28Object.assign%28document.createElement%28%22link%22%29%2C%7Brel%3A%22stylesheet%22%2Chref%3A%22https%3A%2F%2Fnorbert.wip%2Fassets%2Fvendor%2Fhighlight.js%2Fstyles%2Fgithub-dark.min-4b46e20f66f76e35d6454ca4f09b57c3.css%22%7D%29%29",
        "@fontsource-variable/cabin/index.min.css": "data:application/javascript,document.head.appendChild%28Object.assign%28document.createElement%28%22link%22%29%2C%7Brel%3A%22stylesheet%22%2Chref%3A%22https%3A%2F%2Fnorbert.wip%2Fassets%2Fvendor%2F%40fontsource-variable%2Fcabin%2Findex.min-9c7d0ada22af1e85cec470dbedd33409.css%22%7D%29%29",
        "clipboard": "https://norbert.wip/assets/vendor/clipboard/clipboard.index-dcdefe2499a3b69abd4ab17006bfd09b.js",
        "highlight.js/lib/languages/shell": "https://norbert.wip/assets/vendor/highlight.js/lib/languages/shell-82883b411c0363fd67d4b2a65c9191cd.js",
        "highlight.js/lib/languages/json": "https://norbert.wip/assets/vendor/highlight.js/lib/languages/json-9d4f6cc2ef6373562a0ebc9b0e6fe3c3.js",
        "highlight.js/lib/languages/twig": "https://norbert.wip/assets/vendor/highlight.js/lib/languages/twig-68d52b8a7230c15fca667a4297a37ca5.js",
        "highlight.js/lib/languages/sql": "https://norbert.wip/assets/vendor/highlight.js/lib/languages/sql-ba78665514b9cfeb1e8435f38f0e8ab5.js",
        "highlight.js/lib/languages/javascript": "https://norbert.wip/assets/vendor/highlight.js/lib/languages/javascript-ad36fcdcd285bf2c1bee1fef5dbc226f.js",
        "highlight.js/lib/languages/xml": "https://norbert.wip/assets/vendor/highlight.js/lib/languages/xml-283bc16436bf56d2e1178a8f0b733523.js"
    }
}
</script>
<!-- ES Module Shims: Import maps polyfill for modules browsers without import maps support -->
<script async src="https://ga.jspm.io/npm:es-module-shims@1.10.0/dist/es-module-shims.js"></script>
<link rel="modulepreload" href="https://norbert.wip/assets/app-930adf3462cf9ab60908eb1b74cf7ca7.js">
<link rel="modulepreload" href="https://norbert.wip/assets/vendor/@oddbird/popover-polyfill/popover-polyfill.index-8f1adf02d2fe31ba0e133b65d1faeece.js">
<link rel="modulepreload" href="https://norbert.wip/assets/bootstrap-d78d7e12c819dedf89372fb4824c072d.js">
<link rel="modulepreload" href="https://norbert.wip/assets/vendor/htmx.org/htmx.org.index-9c972342e01d7cc4830a1976259158c7.js">
<link rel="modulepreload" href="https://norbert.wip/assets/vendor/iconify-icon/iconify-icon.index-4471e3c6a0bda14753f1327106be2b63.js">
<link rel="modulepreload" href="https://norbert.wip/assets/@symfony/stimulus-bundle/loader-9311b8ea36bad0f6168e687b4d6dee73.js">
<link rel="modulepreload" href="https://norbert.wip/assets/vendor/@hotwired/stimulus/stimulus.index-6f73d85e0fa2bcb3802562288e3f3042.js">
<link rel="modulepreload" href="https://norbert.wip/assets/@symfony/stimulus-bundle/controllers-30e149621baab8220e8ae00cc40a4a23.js">
<link rel="modulepreload" href="https://norbert.wip/assets/controllers/hello_controller-55882fcad241d2bea50276ea485583bc.js">
<script type="module">import 'app';</script>
    </head>
<body class="scroll-smooth text-black relative min-h-screen pb-16">
    <div class="sticky top-0 max-h-screen overflow-y-auto bg-white py-2 px-2 border-b border-gray-500 z-[9999] print:hidden">
        <div class="grid grid-cols-2 sm:mx-auto sm:max-w-screen-2xl md:px-4">
            <div class="text-left">
                <a href="/" class="text-lg">
                    norbert.tech
                </a>
            </div>
            <div class="text-right">
                <a href="/consulting" class="text-lg inline-flex items-center space-x-1 md:mr-4 mr-2">
                    <iconify-icon icon="lineicons:consulting" class="mr-1"></iconify-icon> Consulting
                </a>
                <a href="/blog" class="text-lg inline-flex items-center space-x-1">
                    <iconify-icon icon="ooui:articles-ltr" class="mr-1"></iconify-icon> Blog
                </a>
            </div>
        </div>
    </div>
    
    <main class="mx-auto max-w-screen-2xl mb-4 md:pt-4 px-4 lg:px-0">
            <div class="mx-auto max-w-screen-lg px-2">
        <ul class="mt-2 pl-[20px] flex gap-4">
            <li>
                <a href="/blog" class="text-blue-500 hover:underline">Go Back</a>
            </li>
                                                                <li>
                        <a href="/blog/2025-08-12/pl/analiza-danych-w-rozproszonych-systemach-transakcyjnych"
                           class="text-lg hover:opacity-80"
                           title="Polish">ðŸ‡µðŸ‡± Polish</a>
                    </li>
                                                                                <li>
                        <a href="/blog/2025-08-12/en/data-analytics-in-distributed-transactional-systems"
                           class="text-lg hover:opacity-80"
                           title="English">ðŸ‡ºðŸ‡¸ English</a>
                    </li>
                                                                                <li>
                        <a href="/blog/2025-08-12/fr/analyse-donnees-systemes-transactionnels-distribues"
                           class="text-lg hover:opacity-80"
                           title="French">ðŸ‡«ðŸ‡· French</a>
                    </li>
                                                                                <li>
                        <a href="/blog/2025-08-12/es/analisis-datos-sistemas-transaccionales-distribuidos"
                           class="text-lg hover:opacity-80"
                           title="Spanish">ðŸ‡ªðŸ‡¸ Spanish</a>
                    </li>
                                    </ul>

                    <div class="mt-4 p-4 bg-yellow-50 border border-yellow-200 rounded-lg">
                <div class="flex items-start">
                    <div class="flex-shrink-0">
                        <svg class="h-5 w-5 text-yellow-400" viewBox="0 0 20 20" fill="currentColor">
                            <path fill-rule="evenodd" d="M8.257 3.099c.765-1.36 2.722-1.36 3.486 0l5.58 9.92c.75 1.334-.213 2.98-1.742 2.98H4.42c-1.53 0-2.493-1.646-1.743-2.98l5.58-9.92zM11 13a1 1 0 11-2 0 1 1 0 012 0zm-1-8a1 1 0 00-1 1v3a1 1 0 002 0V6a1 1 0 00-1-1z" clip-rule="evenodd"/>
                        </svg>
                    </div>
                    <div class="ml-3">
                        <h3 class="font-medium text-yellow-800">
                            Translation Notice
                        </h3>
                        <div class="mt-2 text-yellow-700">
                            <p>
                                This is an automatically translated version of that Article. Despite my best efforts, it might not be perfect.<br/>
                                Native speakers are welcome to
                                <a href="https://github.com/norberttech/norbert.tech/edit/main/templates/blog/posts/2025-08-12/datenanalyse-in-verteilten-transaktionssystemen/post.html.twig"
                                   class="underline hover:text-yellow-800" target="_blank" rel="noopener">open pull requests
                                </a> to correct anything that doesn't sound right.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
            </div>
    <article class="blog-post px-2 py-5 sm:px-4 mx-auto max-w-screen-lg">
            <div class="img-wide">
        <img src="https://norbert.wip/assets/images/blog/analytics-in-transactional-distributed-systems/analytics_01-b27f8a842f645a9c7d9901f3fd11fde8.jpg" alt="Datenanalyse in verteilten Transaktionssystemen" />
    </div>

    <h1 class="font-bold text-4xl mb-2" id="title">Datenanalyse in verteilten Transaktionssystemen</h1>
    <div class="mb-2">
        <small class="text-sm">VerÃ¶ffentlicht am August 12, 2025 00:00</small>
    </div>
    <div class="mb-4">
                    <small><span class="badge badge-info">Datenanalyse</span></small>
                    <small><span class="badge badge-info">Data Warehouse</span></small>
                    <small><span class="badge badge-info">ETL</span></small>
                    <small><span class="badge badge-info">Datenverarbeitung</span></small>
                    <small><span class="badge badge-info">Transaktionssysteme</span></small>
            </div>
    <p>
        In diesem Artikel mÃ¶chte ich das Problem der Datenanalyse in verteilten Transaktionssystemen angehen.<br/>
        Wenn Sie nach Ideen fÃ¼r den Aufbau eines zentralen Data Warehouse suchen, das Ihnen ermÃ¶glicht, Daten aus dem gesamten System zu sammeln,
        unabhÃ¤ngig von seiner Fragmentierung und ohne in den Betriebskosten zu ertrinken, dann ist dieser Artikel fÃ¼r Sie.
    </p>

    <h2>Alles beginnt unschuldig</h2>
    <p>
        Die meisten Systeme, die wir tÃ¤glich erstellen, speichern Daten in irgendeiner relationalen Datenbank.
        Eine sehr beliebte und gleichzeitig gute Wahl ist PostgreSQL, das in den letzten Jahren zu einem nahezu
        Standard in der Branche geworden ist.
    </p>
    <p>
        Die Geschichte der meisten Projekte verlÃ¤uft meist sehr Ã¤hnlich: Wir beginnen mit der Verifizierung der Idee, gewinnen
        unsere ersten Benutzer, das System beginnt Geld zu verdienen, das Business Ã¼berlegt, wie man die Gewinne steigern kann,
        neue FunktionalitÃ¤ten entstehen. Jede neue FunktionalitÃ¤t bedeutet einige neue Tabellen in der Datenbank.
    </p>
    <p>
        Um die Entwicklung zu beschleunigen, verwenden wir ein ORM, generieren automatisch Migrationen, die das
        Datenbankschema erstellen und aktualisieren.
    </p>
    <p>
        Anfangs lÃ¤uft alles glatt, neue FunktionalitÃ¤ten bringen die erwarteten Gewinne, das Business beginnt zu skalieren.
        Wir stellen mehr Programmierer ein, um mehr FunktionalitÃ¤ten parallel zu entwickeln.
    </p>
    <p>
        Von Zeit zu Zeit meldet jemand, dass das System an manchen Stellen anfÃ¤ngt zu "hÃ¤ngen", schnelle AufklÃ¤rung, noch
        schnellere Diagnose, es fehlt ein Index in irgendeiner Tabelle.
    </p>
    <p>
        In der ORM-Mapping-Konfiguration fÃ¼gen wir einen Index fÃ¼r das Feld hinzu, nach dem das System sehr hÃ¤ufig Daten sucht.
        Problem gelÃ¶st.
    </p>
    <p>
        Das wachsende Entwicklerteam legt groÃŸen Wert auf QualitÃ¤t, vielleicht verwendet es sogar
        fortgeschrittene Softwareentwicklungstechniken wie Event Storming oder Domain-Driven Design.<br/>
        CI/CD fÃ¼hrt unzÃ¤hlige Tests aus und stellt sicher, dass Ã„nderungen keine Regressionen einfÃ¼hren.
    </p>
    <p>
        Die Idylle dauert an, das Team oder vielleicht mehrere Teams beginnen, neue Module zum System hinzuzufÃ¼gen. Module, die angemessen
        isoliert sind, fÃ¼r spezifische Aufgaben verantwortlich, niemals ihre Grenzen Ã¼berschreiten und nicht in die
        Kompetenzen anderer Module eingreifen.
    </p>
    <p>
        FÃ¼r die Kommunikation werden natÃ¼rlich Warteschlangen verwendet, wir implementieren das
        <a href="https://event-driven.io/en/outbox_inbox_patterns_and_delivery_guarantees_explained/" target="_blank">Outbox/Inbox
            Pattern</a>
    </p>
    <p>
        Um die entsprechende Isolation zu gewÃ¤hrleisten, stellen wir Regeln auf, die besagen, dass jedes Modul nur
        Zugriff auf die Tabellen in der Datenbank hat, die zu ihm gehÃ¶ren. Um Daten aus einem anderen Modul zu erhalten, muss man
        sich an dieses Modul wenden, sei es Ã¼ber eine interne API oder auf andere Weise.
    </p>
    <p>
        Gelegentlich kommt das Business zu uns mit der Frage: <strong>KÃ¶nnt ihr uns schnell diesen
            Bericht erstellen?</strong>
        NatÃ¼rlich, ein paar Zeilen SQL, vielleicht ein paar Dutzend und der Bericht ist fertig.
    </p>
    <p>
        Das Business ist zufrieden, der Bericht als CSV geht zu Excel (dem beliebtesten BI-Tool), das Business zieht
        SchlÃ¼sse, plant neue FunktionalitÃ¤ten und Ã„nderungen.
    </p>
    <div class="img-wide">
        <img src="https://norbert.wip/assets/images/blog/analytics-in-transactional-distributed-systems/happy_business_01-4fb1ffd438d7889256901278c0d60981.jpg" alt="Business Intelligence" />
    </div>

    <h2>Zeit vergeht, neue Tabellen wachsen wie Pilze nach dem Regen</h2>
    <p>
        In diesem Zustand kÃ¶nnen wir sehr lange verharren, sogar mehrere gute Jahre.
    </p>
    <p>
        In der Zwischenzeit wird jemand irgendwo sicherlich auf die Idee kommen, dem System die MÃ¶glichkeit zur Berichtserstellung hinzuzufÃ¼gen.
        Es ist nur eine Frage der Zeit.
    </p>
    <p>
        Berichte Ã¼ber den Systemzustand sind fÃ¼r das Business eines der wichtigsten Werkzeuge, die Einblick in Verhaltensweisen,
        PrÃ¤ferenzen oder Trends der Benutzer geben. Sie ermÃ¶glichen es nicht nur zu verstehen, was passiert, sondern auch angemessen zu planen, was
        erst passieren soll.
    </p>
    <p>
        Je besser und detaillierter die Berichte, desto bessere Entscheidungen kÃ¶nnen auf ihrer Grundlage getroffen werden. Gute
        GeschÃ¤ftsentscheidungen fÃ¼hren zu hÃ¶heren Gewinnen, hÃ¶here Gewinne fÃ¼hren zu einem grÃ¶ÃŸeren Budget.
        Ein grÃ¶ÃŸeres Budget fÃ¼hrt zu besseren Tools, grÃ¶ÃŸeren Teams, besseren GehÃ¤ltern oder Boni.
    </p>
    <p>
        Im Interesse jedes Programmierers sollte es daher sein, dem Business mÃ¶glichst gute und
        prÃ¤zise Daten zu liefern, schlieÃŸlich fÃ¼hren bessere Ergebnisse direkt zu besseren Gewinnen.
    </p>

    <h2>Erste Symptome</h2>
    <p>
        Das System funktioniert, bringt Gewinne. Es besteht aus etwa 5, vielleicht sogar 10 Modulen, jedes Modul besteht aus 20-50
        Tabellen in der Datenbank. Jedes Modul liefert seine eigenen Berichte.
    </p>
    <ul>
        <li>Verkauf</li>
        <li>Marketing</li>
        <li>Logistik</li>
        <li>LagerbestÃ¤nde</li>
        <li>Benutzer</li>
    </ul>
    <p>
        Jedes Modul stellt nur einen Teil der Daten zur VerfÃ¼gung, einen Bruchteil des grÃ¶ÃŸeren Bildes, keines gibt jedoch einen Ãœberblick Ã¼ber das Ganze.
    </p>
    <p>
        Die Teams haben zwar ReferenzschlÃ¼ssel zu Daten aus anderen Modulen implementiert, es gelang sogar,
        in der BenutzeroberflÃ¤che einen Ort zu schaffen, von dem aus Berichte generiert werden kÃ¶nnen.
    </p>
    <p>
        Das ist jedoch immer noch zu wenig...
    </p>
    <p>
        Sehr schnell stellt sich heraus, dass Berichte, die in verschiedenen Modulen generiert werden, von verschiedenen Programmierern geschrieben,
        vielleicht sogar in verschiedenen Technologien, unterschiedliche Datenformate und verschiedene Namensstandards haben.
    </p>
    <p>
        Datumsbereiche werden unterschiedlich interpretiert, ein Modul berÃ¼cksichtigt End- und Anfangsdaten, ein anderes schlieÃŸt sie aus,
        und ein anderer macht ein rechtsseitig offenes Intervall zur Erleichterung der Paginierung, weil sie zufÃ¤llig
        auch eine API haben und diese API denselben Code-Teil verwendet.
    </p>
    <p>
        Da jedes Modul unabhÃ¤ngig ist, seine eigenen Grenzen und seine eigene Nomenklatur hat, stellen wir irgendwann fest,
        dass das, was wir in einem Modul irgendwie nennen, ein anderes Modul unter einem vÃ¶llig anderen Namen exponiert.
        Weil es im Kontext dieses Moduls Sinn macht.
    </p>
    <p>
        Mit der Zeit werden wir wahrscheinlich auch feststellen, dass jedes Team unterschiedlich seine Retention- und
        Datenspeicherrichtlinien definiert hat.
        Trotz des Besitzes von Daten der letzten 5 Jahre im SchlÃ¼sselmodul kÃ¶nnen wir nichts mit ihnen anfangen, weil Module, die
        Daten zur Anreicherung des Grundberichts lieferten, nur Daten der letzten 2 Jahre besitzen.
    </p>
    <p>
        Dies sind jedoch keine Probleme, die ein wenig Excel-Magie nicht lÃ¶sen kÃ¶nnte (vielleicht abgesehen von fehlenden
        Daten).
        Wir Ã¤ndern die Namen dieser Spalten, entfernen jene, fÃ¼gen schnelle Filter hinzu und das reicht.
    </p>
    <p>
        Wir erstellen eine groÃŸe Datei, in der wir ein Arbeitsblatt namens "Dashboard" haben, und alle
        anderen sind nur lesbar und versorgen das Dashboard.
    </p>
    <p>
        Vielleicht funktioniert dieser Ansatz sogar eine Weile. Vielleicht sogar lÃ¤nger als eine Weile, aber machen wir uns keine Illusionen.
        Das alles wird schlieÃŸlich zusammenbrechen, und das gemÃ¤ÃŸ <a href="https://en.wikipedia.org/wiki/Murphy%27s_law" target="_blank">Murphys Gesetz</a>
        im schlimmstmÃ¶glichen Moment.
    </p>

    <h2>Was ist schlecht an Excel?</h2>
    <p>
        Nichts! Excel ist ein fantastisches Tool. Das Problem liegt nicht in Excel, sondern in seiner Verwendung.
    </p>
    <p>
        Diese ganze Magie der Datenreinigung und -aufbereitung sollte nicht in Excel stattfinden, nicht
        in grÃ¶ÃŸerem MaÃŸstab. Wenn wir Ã¼ber einen einmaligen schnellen Bericht sprechen, ist das kein Problem. Wir tun, was wir mÃ¼ssen,
        klimpern Formeln, analysieren Daten und vergessen.
    </p>
    <p>
        Wenn dies jedoch Teil unserer tÃ¤glichen Routine werden soll, wenn wir zyklisch durch denselben
        Prozess gehen mÃ¼ssen, folgend den stÃ¤ndigen Ã„nderungen und der Evolution des Systems, wird sich frÃ¼her oder spÃ¤ter herausstellen, dass diese ArbeitsblÃ¤tter
        veraltet sind.
    </p>
    <p>
        Spalten existieren nicht mehr oder haben ihre Namen geÃ¤ndert, neue Spalten sind entstanden, das Datenformat hat sich geÃ¤ndert oder
        schlimmer noch, eines der Teams, das fÃ¼r eines der Module verantwortlich ist, hat einige Daten ohne das Bewusstsein gelÃ¶scht, dass sie
        von einem GeschÃ¤ftsbenutzer irgendwo in einem seiner Berichte verwendet wurden, den er einmal im Quartal Ã¶ffnet.
    </p>
    <p>
        Auf lange Sicht sind komplexere Tabellenkalkulationen, die Daten aus automatisch vom System generierten
        Berichten beziehen, die dann basierend auf impliziten Regeln zusammengefÃ¼gt werden, nicht wartbar.
    </p>

    <h2>Sollen wir vielleicht ein BI-Tool anschlieÃŸen?</h2>
    <p>
        Dachten viele Programmierer, die mehrfach mit dem Problem der Berichtserstellung konfrontiert waren.
    </p>
    <p>
        Nehmen wir zum Beispiel <a href="https://www.metabase.com/" target="_blank">Metabase</a>. Ein kostenloses Tool,
        das wir in wenigen Minuten mit Docker aufsetzen kÃ¶nnen.
    </p>
    <p>
        Geben Sie ihm Zugriff auf unsere Datenbank und einige oder alle Tabellen, und Ã¼ber eine sehr benutzerfreundliche OberflÃ¤che
        kann das Business sehr einfach und angenehm die kompliziertesten Berichte generieren.
    </p>
    <p>
        Berichte, die Daten aus mehreren Modulen gleichzeitig enthalten kÃ¶nnen!
    </p>
    <p>
        Wir kÃ¶nnen sogar einen Datenanalysten mit SQL-Grundkenntnissen einstellen, der alles das, was sich nicht
        anklicken lÃ¤sst, mit entsprechend vorbereiteten Abfragen erreicht.
    </p>

    <h2>Nur, das lÃ¶st das Problem nicht</h2>
    <p>
        Es verschiebt es nur in der Zeit.
    </p>
    <p>
        Wenn wir genau hinschauen, was sich geÃ¤ndert hat, dann hat sich nur eine Sache geÃ¤ndert. Das Tool...
        Wir haben das Problem der Datenreinigung und -verknÃ¼pfung von Excel zu Metabase verschoben.
    </p>
    <p>
        Excel ist zwar zu seiner ursprÃ¼nglichen Rolle zurÃ¼ckgekehrt, wir kÃ¶nnen jetzt Berichte, die aus Metabase heruntergeladen wurden, in Excel werfen.
    </p>
    <p>
        Jedoch ist unsere implizite Logik zur DatenverknÃ¼pfung/-reinigung von der Tabellenkalkulation zu SQL-Abfragen
        gewandert.
    </p>
    <p>
        DarÃ¼ber hinaus sind alle Probleme dieselben geblieben:
    </p>
    <ul>
        <li>Dateninkonsistenz</li>
        <li>Inkonsistente Namensgebung</li>
        <li>Fehlen einer einheitlichen RÃ¼ckwÃ¤rtskompatibilitÃ¤tsrichtlinie</li>
        <li>Fehlen einer einheitlichen Datenaufbewahrungsrichtlinie</li>
    </ul>

    <h2>Sollen wir vielleicht Prozesse und Regeln einfÃ¼hren?</h2>
    <p>
        Die meisten der oben genannten Probleme lassen sich durch die Implementierung entsprechender Prozesse und Regeln lÃ¶sen.
    </p>
    <p>
        Wir kÃ¶nnen Namensstandards festlegen, die besagen, dass jede Tabelle in der Datenbank ein ModulprÃ¤fix im Namen enthalten muss, und
        Spalten in Kleinbuchstaben und durch Unterstriche getrennt benannt werden.
    </p>
    <p>
        Wir kÃ¶nnen festlegen, dass jedes Modul Daten der letzten 5 Jahre speichert (Hot Storage), alles Ã„ltere wird
        archiviert. (Cold Storage)
    </p>
    <p>
        Wir kÃ¶nnen festlegen, dass Datumsbereiche immer als rechtsseitig offene Intervalle behandelt werden.
    </p>
    <p>
        Wir kÃ¶nnen festlegen, dass wir keine Spalten aus der Datenbank entfernen, oder dass wir vor dem Entfernen von etwas zuerst
        in eine Ãœbergangszeit eintreten, wÃ¤hrend der wir jedem Systembenutzer zeigen,
        welche Spalten sich Ã¤ndern werden und wie.
    </p>
    <p>
        Selbst wenn wir fÃ¼r die Diskussion annehmen, dass es gelingt, diese Prozesse global zwischen mehreren
        Teams zu implementieren und dass diese Teams sie bedingungslos und sehr genau befolgen werden, <strong>ist das nicht genug...</strong>
    </p>

    <h2>Datenbankskalierung ist nicht billig</h2>
    <p>
        Besonders wenn wir uns auf Cloud-LÃ¶sungen stÃ¼tzen.
    </p>
    <p>
        Stellen wir uns eine Situation vor, in der wÃ¤hrend der Spitzenarbeitszeiten des Systems (wenn Benutzer die meisten
        Transaktionen generieren) ein GeschÃ¤ftsanalyst, der nach seinem eigenen Plan arbeitet, einen Bericht basierend auf einem typischen SQL-
        Mehrkilozeiler generieren muss?
    </p>
    <p>
        Der Analyst startet die Abfrage, die Datenbank beginnt zu mahlen. Die Abfrage dauert 5, 10, 15 Minuten.
        Die Datenbank beginnt zu schwitzen.
    </p>
    <p>
        Benutzer bombardieren das System mit neuen Bestellungen (oder anderen Operationen, die viele
        SchreibvorgÃ¤nge generieren), wÃ¤hrend der Analyst auf Ergebnisse wartet.
    </p>
    <p>
        Gleichzeitig muss jemand aus dem Business schnell ein paar Berichte Ã¼berprÃ¼fen, jeder enthÃ¤lt
        "die Gesamtzahl der Zeilen in der Tabelle".
        Es gibt mehrere solcher Personen.
    </p>
    <p>
        All diese Operationen Ã¼berlagern sich, unsere bereits sehr belastete Datenbank schafft es nicht.
    </p>
    <p>
        Einige Benutzertransaktionen kommen nicht durch. <br/>
        Das System atmet kaum. Die Wartezeit fÃ¼r grundlegendste Operationen wird in Sekunden gemessen.
    </p>
    <p>
        Und jetzt das SahnehÃ¤ubchen: Wenn all diese Danteske Szenen stattfinden, wenn Pager Duty glÃ¼hend heiÃŸ ist
        von allen Arten von VorfÃ¤llen, wenn Teams in Panik versuchen, das System wieder zum Leben zu erwecken,
        DevOps-Leute Ã¼berlegen, wie man die Datenbank schnell skaliert...
    </p>
    <div class="img-wide">
        <img class="mt-[-150px]" src="https://norbert.wip/assets/images/blog/analytics-in-transactional-distributed-systems/construction_01-59e51f16aa79ecadc241f490217f29a0.jpg" alt="Renovierungsarbeiten" />
    </div>
    <p>
        Der CEO beginnt eine PrÃ¤sentation fÃ¼r einen potenziellen GeschÃ¤ftspartner, mit dem die Zusammenarbeit
        sich als entscheidend fÃ¼r die Unternehmensstrategie erweisen soll...
    </p>

    <h2>Sollen wir einfach ein Replikat aufsetzen?</h2>
    <p>
        SchlieÃŸlich werden Berichte unsere Transaktionsdatenbank nicht Ã¼berlasten.
    </p>
    <p>
        Wir verdoppeln zwar die Datenbankunterhaltungskosten, reduzieren aber das SystemÃ¼berlastungsrisiko und kÃ¶nnen
        unser liebstes Business Intelligence Tool direkt an das Replikat anschlieÃŸen, was uns Echtzeitdaten gibt.
    </p>
    <p>
        Klingt fantastisch, aber in der Praxis ist es nicht so einfach.
    </p>
    <p>
        Abgesehen von potenziellen Problemen, die sich aus der Natur der Replikation selbst ergeben, ist das Haupt- und Grundproblem,
        auf das ich am hÃ¤ufigsten stoÃŸe, die <strong>Wahrnehmung</strong>.
    </p>
    <p>
        VÃ¶llig anders werden ein Programmierer, der diese Tabellen mit ORM-Mappings generiert hat, und ein Datenanalyst
        auf Tabellen in der Datenbank blicken.
    </p>
    <p>
        Der Programmierer wird wissen, welche Tabellen miteinander verbunden werden mÃ¼ssen, um ein Gesamtbild zu erhalten.
        Er wird die EinschrÃ¤nkungen und Bedingungen verstehen, die irgendwo im Anwendungscode vergraben sind.
        Vor allem kennt der Programmierer den Lebenszyklus des Systems (seiner Daten) oder sollte sich zumindest orientieren.
    </p>
    <p>
        All dieses Wissen ist fÃ¼r Analysten meist nicht verfÃ¼gbar.
    </p>
    <p>
        Es ist, als wÃ¼rde man jemandem sagen, er solle durch ein SchlÃ¼sselloch schauen. Etwas kann man sicherlich sehen.
        Einige SchlÃ¼sse lassen sich ziehen, aber es wird sehr schwer sein, das Ganze zu rekonstruieren.
    </p>
    <p>
        Es reicht, dass wir eine JSONB-Spalte in der Datenbank haben, in der wir einige Datenstrukturen speichern.
        Nehmen wir an, das System erlaubt 3 gÃ¼ltige Kombinationen derselben Struktur, aber eine ist superselten, so
        selten, dass sie im System noch nicht aufgetreten ist. Beim Betrachten der Daten, auch ganzheitlich, kann der Analyst einfach nicht wissen,
        dass 3 Kombinationen einer Struktur existieren. Bei der Normalisierung wird er 2 FÃ¤lle berÃ¼cksichtigen, wÃ¤hrend der dritte
        zu einer tickenden Zeitbombe wird, die wie immer im unerwartesten Moment explodiert.
    </p>
    <p>
        Anders gesagt, wenn wir mehrere unabhÃ¤ngige Module im System haben. Jedes mit seiner eigenen Datenbank oder zumindest
        seinen eigenen Tabellen in der Datenbank. Was zusammen 200-300 Tabellen ergibt, ist die Erwartung, dass der Analyst das ohne Probleme
        bewÃ¤ltigt, keine Fehler macht und Berichte nicht von den Erwartungen abweichen, gelinde gesagt naiv.
    </p>
    <p>
        Trotz allem ist das Aufstellen einer Kopie/Replikat-Datenbank fÃ¼r Analysten und die Vergabe eines 4-buchstabigen Namens, der vom
        Wort "Analytics" stammt, immer noch weit verbreitet.
    </p>
    <p>
        BI-Tools Ã¼berbieten sich darin, wer die bessere BenutzeroberflÃ¤che erstellt, mit der sich Berichte anklicken lassen.
        Sie versprechen, dass wir Daten ohne SQL analysieren kÃ¶nnen.
    </p>
    <p>
        Ja, das kann funktionieren, an vielen Stellen funktioniert es genau so. WorÃ¼ber wir jedoch nicht laut sprechen:
    </p>
    <ul>
        <li>Probleme mit RÃ¼ckwÃ¤rtskompatibilitÃ¤t und DatenstrukturÃ¤nderungen</li>
        <li>Probleme mit der ordnungsgemÃ¤ÃŸen Wartung/Versionierung/Tests von gigantischen SQL-Abfragen/Skripten zur
            Datennormalisierung im laufenden Betrieb
        </li>
        <li>Replikate/Kopien erzeugen zusÃ¤tzliche Kosten</li>
        <li>Die Reduzierung der Replikatressourcen ist entweder unmÃ¶glich oder macht die Berichtserstellung in akzeptablen
            Zeiten unmÃ¶glich
        </li>
    </ul>
    <p>
        Was sich letztendlich auf die DatenqualitÃ¤t und die EffektivitÃ¤t der GeschÃ¤ftsentscheidungen auswirkt.
    </p>

    <h2>Was bleibt uns Ã¼brig?</h2>
    <p>
        Vielleicht sollten wir zuerst festlegen, welche Probleme wir in erster Linie lÃ¶sen wollen:
    </p>
    <div class="img-wide">
        <img src="https://norbert.wip/assets/images/blog/analytics-in-transactional-distributed-systems/strategy_01-f82682bfa13643ba5b8957e806e0d823.jpg" alt="Strategie und Analyse" />
    </div>
    <ol>
        <li>Datenanalyse/Berichtserstellung darf keinen Einfluss auf die Systemfunktion haben.</li>
        <li>Daten in Berichten mÃ¼ssen immer aktuell sein (DatenverzÃ¶gerung ist akzeptabel, individuell festgelegt)</li>
        <li>Berichte mÃ¼ssen den realen, unverzerrten Systemzustand widerspiegeln</li>
        <li>Die Datenstruktur muss regressionsresistent sein</li>
        <li>Einheitliche Datenaufbewahrungs- und Archivierungsrichtlinie</li>
    </ol>

    <h2>1) Ressourcentrennung</h2>
    <p>
        Das ist nichts RevolutionÃ¤res, wenn wir nicht wollen, dass unser System durch Ãœberlastung der Datenbank
        durch Berichtserstellung gefÃ¤hrdet wird, mÃ¼ssen wir eine separate Datenbank aufsetzen.
    </p>
    <p><strong>Welche Datenbank fÃ¼r Analytics wÃ¤hlen?</strong></p>
    <p>
        Das ist im Grunde ein Thema fÃ¼r einen separaten Artikel oder sogar eine Artikelserie.
        Es gibt sehr viele LÃ¶sungen, einige bessere, andere schlechtere. Es gibt keine einzige
        magische LÃ¶sung fÃ¼r alle Probleme.
    </p>
    <p>
        Mein Rat, besonders fÃ¼r kleinere Teams ohne Erfahrung im Datenmanagement, ist, sich nicht
        auf Technologien zu stÃ¼rzen, mit denen wir keine Erfahrung haben.
    </p>
    <p>
        Entscheidend ist das richtige Datenformat. Nach der Umwandlung vieler schmaler Tabellen in eine breite wird sich wahrscheinlich
        herausstellen, dass die Generierung desselben Berichts ohne 20x <code>JOIN</code> nicht mehr 10 Minuten
        sondern weniger als eine halbe Sekunde dauert.
    </p>
    <p>
        Und wenn das Problem Aggregationen sind, nicht VerknÃ¼pfungen?
    </p>
    <p>
        Dann ist es besser, anstatt im laufenden Betrieb zu aggregieren, eine Tabelle vorzubereiten, die diese Daten in aggregierter
        und nicht in roher Form enthÃ¤lt.
    </p>

    <h2>2) Aktuelle Daten</h2>
    <p>
        Nun gut, aber wenn wir eine neue, unabhÃ¤ngige Datenbank erstellen, wie sorgen wir dafÃ¼r, dass die Daten in dieser
        Datenbank aktuell und frisch sind?
    </p>
    <p>
        Hier hÃ¤ngt sehr viel von der akzeptablen VerzÃ¶gerung bei der Datensynchronisation ab.
        Meist reicht es, wenn die Analysedatenbank etwa 24 Stunden hinter der Transaktionsdatenbank liegt. Das heiÃŸt, sie enthÃ¤lt
        Daten bis "gestern", einschlieÃŸlich des ganzen "gestrigen Tages".
    </p>
    <p>
        Warum? Weil nur wenige GeschÃ¤ftsentscheidungen sofort getroffen werden.
        Wenn Entscheidungen in so kurzer Zeit getroffen werden mÃ¼ssen, dann baut man entsprechende Automatisierungen.
    </p>
    <p>
        Wenn eine 24-stÃ¼ndige VerzÃ¶gerung akzeptabel ist (manchmal ist sie es nicht und dafÃ¼r gibt es auch LÃ¶sungen),
        reicht es, wenn wir die Synchronisation mehrmals tÃ¤glich durchfÃ¼hren.
        NatÃ¼rlich gibt es auch hier keine goldene Regel. Genauso wie es keine Regel gibt, die besagt, welchen Bereich man auf einmal synchronisieren soll.
    </p>
    <p>
        Es gibt jedoch eine gute Praxis, die die Synchronisation erleichtert. Sie besteht darin, sicherzustellen, dass die Haupttabellen im
        Transaktionssystem das Datum der Erstellung/Ã„nderung des Datensatzes enthalten.
    </p>
    <p>
        Mit diesen beiden Informationen kÃ¶nnen wir das Synchronisationsfenster auf einen bestimmten Zeitraum eingrenzen.
    </p>
    <p>
        Wie sieht das in der Praxis aus? Wir kÃ¶nnen z.B. alle 6 Stunden einen Synchronisationsprozess starten und nur DatensÃ¤tze sammeln, die in den
        letzten 24 Stunden geÃ¤ndert wurden.<br/>
        <code>Das sind natÃ¼rlich Beispielzahlen, diese Werte mÃ¼ssen basierend auf der GrÃ¶ÃŸe und dem Verhalten der Daten festgelegt werden.</code>
    </p>
    <p>
        Warum aus 24 Stunden? Als zusÃ¤tzliche Sicherheit. Wir kÃ¶nnten Daten nur aus 7 Stunden holen, aber wenn aus irgendeinem
        Grund die Synchronisation nicht ausgefÃ¼hrt wird und wir das nicht bemerken, kÃ¶nnten wir Daten verlieren.
    </p>

    <h2>3) Systemzustand widerspiegeln</h2>
    <p>
        Meine Meinung zu diesem Thema mag kontrovers erscheinen, aber ich glaube, dass das beste Wissen Ã¼ber Daten und Systemverhalten
        das Team hat, das dieses System/Modul baut.
    </p>
    <p>
        Genau dieses Team sollte dafÃ¼r verantwortlich sein, dass Daten, die vom System oder seinem Teil generiert werden,
        fÃ¼r den das gegebene Team verantwortlich ist, in das zentrale Datenrepository gelangen.
    </p>
    <p>
        Mit anderen Worten, genau das Team, das eine bestimmte FunktionalitÃ¤t implementiert, sollte basierend auf zuvor gesammelten Anforderungen
        diese Daten in das entsprechende Format umwandeln und weiterleiten.
    </p>
    <p>
        Dies ist wahrscheinlich der einfachste Weg sicherzustellen, dass die Daten vollstÃ¤ndig sind und Programmierer aus dem jeweiligen Team sich
        bewusst sind, dass diese Daten irgendwo verwendet werden. Das analytische Datenformat wird fÃ¼r sie zu
        einer Art Vertrag â€“ einem Vertrag, den sie einhalten mÃ¼ssen.
    </p>
    <p>
        Das unterscheidet sich nicht sehr vom API-Schema-Vertrag.
    </p>

    <h2>4) Regressionsresistenz</h2>
    <p>
        Dieser Punkt ist wahrscheinlich der komplizierteste. Die korrekte Implementierung der Datenschema-Evolution ist
        oft nicht so sehr schwierig als vielmehr mÃ¼hsam.
    </p>
    <p>
        In Kurzform sehen die Regeln so aus:
    </p>
    <ul>
        <li>Wir entfernen niemals Spalten</li>
        <li>Alle Spalten, die wir hinzufÃ¼gen, mÃ¼ssen <code>nullable</code> sein oder einen Standardwert haben</li>
        <li>Spaltentypen kÃ¶nnen wir nur erweitern, zum Beispiel kÃ¶nnen wir <code>int</code> in <code>bigint</code> umwandeln, aber nicht umgekehrt</li>
        <li>Wir Ã¤ndern keine Spaltennamen</li>
    </ul>
    <p>
        KÃ¶nnen wir also nichts lÃ¶schen?
    </p>
    <p>
        KÃ¶nnen wir, aber nicht einfach so. Generell hÃ¤ngt es nur von uns ab, wie und wie oft wir die RÃ¼ckwÃ¤rtskompatibilitÃ¤t brechen.
    </p>
    <p>
        Wenn wir unsere analytische Datenquelle nur intern nutzen und, sagen wir, der Analyst, der Berichte erstellt,
        auf dem Laufenden mit SystemÃ¤nderungen ist, kÃ¶nnten wir bei entsprechender Koordination neue Tabellen hinzufÃ¼gen
        und dann alte entfernen, ihm etwas Zeit geben, die Berichte zu aktualisieren.
    </p>
    <p>
        Wenn jedoch unsere analytische Datenquelle fÃ¼r <code>Data Science</code> verwendet wird, aber wir in einer
        Multi-Tenancy-Umgebung arbeiten und analytische Daten/Berichte Kunden zur VerfÃ¼gung gestellt werden, dann mÃ¼ssen wir vÃ¶llig anders an die Sache herangehen.
    </p>

    <h2>Datenaufbewahrungs- und Archivierungsrichtlinie</h2>
    <p>
        Wie ich oben erwÃ¤hnt habe, ist es sehr wichtig, dass Daten in der Analysedatenbank, insbesondere die von verschiedenen
        Modulen gelieferten, denselben Regeln bezÃ¼glich der Aufbewahrungszeit unterliegen.
    </p>
    <p>
        Wenn wir LagerzustÃ¤nde im System nur aus dem letzten Jahr speichern, aber Bestellungen aus den letzten 5 Jahren,
        kÃ¶nnen Analysten keinen Bericht erstellen, der Daten aus beiden Quellen enthÃ¤lt.
    </p>
    <p>
        Das ist eher ein formales als ein technisches Problem. Es scheint, dass es ausreichen wÃ¼rde, sich einfach zu einigen,
        aber in der Praxis ist es nicht so einfach.
    </p>
    <p>
        Um eine gemeinsame Datenaufbewahrungs- und Archivierungsrichtlinie festzulegen, mÃ¼ssen nicht nur technische,
        sondern auch rechtliche, geschÃ¤ftliche oder analytische Aspekte berÃ¼cksichtigt werden, was Kompromisse erfordern kann.
    </p>

    <h2>Beispiele</h2>
    <p>
        Schauen wir uns nun ein einfaches Beispiel eines ETL-Prozesses an, dessen Aufgabe es ist, Daten von der Transaktionsdatenbank
        zur Analysedatenbank zu Ã¼bertragen.
    </p>
    <blockquote>
        In diesem Beispiel verwende ich <a href="https://flow-php.com" target="_blank">Flow PHP</a>, das ist
        jedoch nichts speziell Einzigartiges fÃ¼r PHP. In jeder Sprache kÃ¶nnen wir etwas sehr Ã„hnliches mit
        jeder Bibliothek erstellen, die die Erstellung von CLI-Anwendungen und einem Tool zur Datenverarbeitung erleichtert.
    </blockquote>
    <p>
        Das folgende Beispiel (in etwas verÃ¤nderter Form) stammt aus einer Live-Stream-Session, die ich das VergnÃ¼gen hatte, mit Roland aufzunehmen, der den Kanal <a href="https://nevercodealone.de/de" target="_blank">Never Code Alone</a> betreibt.
        Das Videomaterial finden Sie auf YouTube unter dem Begriff "Flow PHP"
    </p>
    <p>
        Nehmen wir an, dass das Bestellformat etwa so aussieht:
    </p>
    <pre><code class="code-shell" data-controller="syntax-highlight">schema
|-- order_id: ?uuid
|-- seller_id: uuid
|-- created_at: datetime
|-- updated_at: datetime
|-- cancelled_at: ?datetime
|-- discount: ?float
|-- email: string
|-- customer: string
|-- address: map&lt;string, string&gt;
|-- notes: list&lt;string&gt;
|-- items: list&lt;structure{sku: string, quantity: integer, price: float}&gt;</code></pre>

    <p>
        Unser Ziel ist es, diese Bestellungen in die Analysedatenbank zu Ã¼bertragen, also bereiten wir das Schema der Eingabedaten
        sowie der Zieldaten vor.
    </p>

    <pre><code class="code-php" data-controller="syntax-highlight">&lt;?php

declare(strict_types=1);

namespace App\DataFrames;

use Flow\ETL\Adapter\Doctrine\DbalMetadata;
use function Flow\ETL\DSL\integer_schema;
use function Flow\ETL\DSL\uuid_schema;
use function Flow\ETL\DSL\datetime_schema;
use function Flow\ETL\DSL\float_schema;
use function Flow\ETL\DSL\string_schema;
use function Flow\ETL\DSL\map_schema;
use function Flow\Types\DSL\type_map;
use function Flow\Types\DSL\type_string;
use function Flow\ETL\DSL\list_schema;
use function Flow\Types\DSL\type_list;
use function Flow\Types\DSL\type_structure;
use function Flow\Types\DSL\type_integer;
use function Flow\Types\DSL\type_float;
use function \Flow\ETL\DSL\schema;
use Flow\ETL\Schema;

final class Orders
{
    public static function sourceSchema() : Schema
    {
        return schema(
            uuid_schema(&quot;order_id&quot;),
            uuid_schema(&quot;seller_id&quot;),
            datetime_schema(&quot;created_at&quot;),
            datetime_schema(&quot;updated_at&quot;, nullable: true),
            datetime_schema(&quot;cancelled_at&quot;, nullable: true),
            float_schema(&quot;discount&quot;, nullable: true),
            string_schema(&quot;email&quot;),
            string_schema(&quot;customer&quot;),
            map_schema(&quot;address&quot;, type: type_map(key_type: type_string(), value_type: type_string())),
            list_schema(&quot;notes&quot;, type: type_list(element: type_string())),
            list_schema(&quot;items&quot;, type: type_list(element: type_structure(elements: [&quot;item_id&quot; =&gt; type_string(), &quot;sku&quot; =&gt; type_string(), &quot;quantity&quot; =&gt; type_integer(), &quot;price&quot; =&gt; type_float()]))),
        );
    }

    public static function destinationSchema() : Schema
    {
        return self::sourceSchema()
            -&gt;replace(&#039;updated_at&#039;, datetime_schema(&quot;updated_at&quot;))
            -&gt;remove(&#039;address&#039;)
            -&gt;add(
                string_schema(&#039;street&#039;, metadata: DbalMetadata::length(2048)),
                string_schema(&#039;city&#039;, metadata: DbalMetadata::length(512)),
                string_schema(&#039;zip&#039;, metadata: DbalMetadata::length(32)),
                string_schema(&#039;country&#039;, metadata: DbalMetadata::length(128)),
            )
            -&gt;remove(&#039;items&#039;)
            -&gt;add(
                uuid_schema(&#039;item_id&#039;, metadata: DbalMetadata::primaryKey()),
                string_schema(&#039;sku&#039;, metadata: DbalMetadata::length(64)),
                integer_schema(&#039;quantity&#039;),
                integer_schema(&#039;price&#039;),
                string_schema(&#039;currency&#039;, metadata: DbalMetadata::length(3)),
            )
            ;
    }
}</code></pre>

    <p>
        Beachten Sie, dass die Zielstruktur der Tabelle nicht mehr auf Bestellungen ausgerichtet ist, sondern auf bestellte Artikel.
        Unser Ziel ist es, die Bestellartikel so zu entpacken, dass jeder eine separate Zeile ist.
    </p>
    <p>
        Dadurch muss der Analyst, der einen Bericht generieren muss, nicht mehr kombinieren und
        JSON im laufenden Betrieb entpacken.
    </p>
    <p>
        Die Adressspalte wurde auch in mehrere Spalten aufgeteilt, wodurch der Bericht einfacher
        gefiltert werden kann.
    </p>
    <p>
        Eine weitere wichtige Transformation ist die Umwandlung von <code>price</code> von <code>float</code> in <code>int</code>
        durch Multiplikation des Gleitkommawerts mit 100.
    </p>
    <p>
        Die letzte Ã„nderung wird das HinzufÃ¼gen von Informationen Ã¼ber die WÃ¤hrung der Preise sein. Aber woher kommt diese Information?
        Das ist ein sehr wichtiges Detail, das aus einer nicht sehr guten Implementierung resultiert.
        In diesem speziellen Fall sind alle Bestellungen in Dollar. Das System weiÃŸ das, die Programmierer wissen das,
        aber eine Person, die die Tabellen in der Datenbank ohne Kontext betrachtet, muss dieses Wissen nicht unbedingt haben.
    </p>
    <p>
        Unsere Zielstruktur sollte etwa so aussehen:
    </p>

    <pre><code class="code-shell" data-controller="syntax-highlight">schema
|-- order_id: uuid
|-- seller_id: uuid
|-- created_at: datetime
|-- updated_at: datetime
|-- cancelled_at: ?datetime
|-- discount: ?float
|-- email: string
|-- customer: string
|-- notes: list&lt;string&gt;
|-- street: string
|-- city: string
|-- zip: string
|-- country: string
|-- item_id: uuid
|-- sku: string
|-- quantity: integer
|-- price: integer
|-- currency: string</code></pre>

    <p>
        Der nÃ¤chste Schritt ist die Erstellung der entsprechenden Tabelle in der Analysedatenbank. Das kÃ¶nnen wir relativ
        einfach mit dem Adapter fÃ¼r Doctrine DBAL erreichen.
    </p>

    <pre><code class="code-php" data-controller="syntax-highlight">&lt;?php

declare(strict_types=1);

namespace App\Dbal;

use App\DataFrames\Orders;
use App\DataFrames\OrdersCSV;
use Doctrine\DBAL\Schema\Schema;
use Doctrine\Migrations\Provider\SchemaProvider as MigrationsSchemaProvider;
use function Flow\ETL\Adapter\Doctrine\to_dbal_schema_table;

final class SchemaProvider implements MigrationsSchemaProvider
{
    public const ANALYTICAL_ORDER_LINE_ITEMS = &#039;order_line_items&#039;;

    public function createSchema(): Schema
    {
        return new Schema(
            tables: [
                to_dbal_schema_table(Orders::destinationSchema(), self::ANALYTICAL_ORDER_LINE_ITEMS),
            ]
        );
    }
}</code></pre>

    <p>
        In der Analysedatenbank werden wir also eine "vereinfachte" oder "normalisierte" Version der Bestelltabelle speichern.
        Die Normalisierung besteht im Entpacken der Bestellartikel und ihrer Umwandlung in separate Zeilen sowie
        der Aufteilung der "Adress"-Spalte in mehrere Spalten.
    </p>

    <p>
        Schauen wir uns also den CLI-Befehl an, der fÃ¼r die Ãœbertragung von Daten von der Transaktions-
        zur Analysedatenbank verantwortlich ist.
    </p>

    <pre><code class="code-php" data-controller="syntax-highlight">&lt;?php

namespace App\Command;

use App\DataFrames\Orders;
use App\DataFrames\OrdersCSV;
use App\Dbal\SchemaProvider;
use Doctrine\DBAL\Connection;
use Flow\Doctrine\Bulk\Dialect\SqliteInsertOptions;
use Flow\ETL\Rows;
use Symfony\Component\Console\Attribute\AsCommand;
use Symfony\Component\Console\Command\Command;
use Symfony\Component\Console\Helper\TableSeparator;
use Symfony\Component\Console\Input\InputInterface;
use Symfony\Component\Console\Input\InputOption;
use Symfony\Component\Console\Output\OutputInterface;
use Symfony\Component\Console\Style\SymfonyStyle;
use function Flow\ETL\Adapter\Doctrine\from_dbal_key_set_qb;
use function Flow\ETL\Adapter\Doctrine\pagination_key_desc;
use function Flow\ETL\Adapter\Doctrine\pagination_key_set;
use function Flow\ETL\Adapter\Doctrine\to_dbal_table_insert;
use function Flow\ETL\DSL\analyze;
use function Flow\ETL\DSL\constraint_unique;
use function Flow\ETL\DSL\data_frame;
use function Flow\ETL\DSL\lit;
use function Flow\ETL\DSL\ref;
use function Flow\ETL\DSL\rename_replace;
use function Flow\ETL\DSL\schema_to_ascii;
use function Flow\Types\DSL\type_datetime;

#[AsCommand(
    name: &#039;app:orders:import&#039;,
    description: &#039;Import orders from the transactional database to the analytical database.&#039;,
)]
class OrdersImportCommand extends Command
{
    public function __construct(
        private readonly Connection $transactional,
        private readonly Connection $analytical,
    )
    {
        parent::__construct();
    }

    protected function configure()
    {
        $this-&gt;addOption(&#039;start-date&#039;, null, InputOption::VALUE_REQUIRED, &#039;Start date for the data pull.&#039;, &#039;-24 hours&#039;)
            -&gt;addOption(&#039;end-date&#039;, null, InputOption::VALUE_REQUIRED, &#039;End date for the data pull.&#039;, &#039;now&#039;)
        ;
    }


    protected function execute(InputInterface $input, OutputInterface $output): int
    {
        $io = new SymfonyStyle($input, $output);

        $io-&gt;title(&#039;Importing orders&#039;);

        $startDate = type_datetime()-&gt;cast($input-&gt;getOption(&#039;start-date&#039;));
        $endDate = type_datetime()-&gt;cast($input-&gt;getOption(&#039;end-date&#039;));

        $io-&gt;progressStart();

        $report = data_frame()
            -&gt;read(
                from_dbal_key_set_qb(
                    $this-&gt;transactional,
                    $this-&gt;transactional-&gt;createQueryBuilder()
                        -&gt;select(&#039;*&#039;)
                        -&gt;from(SchemaProvider::ORDERS)
                        -&gt;where(&#039;updated_at BETWEEN :start_date AND :end_date&#039;)
                        -&gt;setParameter(&#039;start_date&#039;, $startDate-&gt;format(&#039;Y-m-d H:i:s&#039;))
                        -&gt;setParameter(&#039;end_date&#039;, $endDate-&gt;format(&#039;Y-m-d H:i:s&#039;)),
                    pagination_key_set(
                        pagination_key_desc(&#039;updated_at&#039;),
                        pagination_key_desc(&#039;order_id&#039;)
                    )
                )-&gt;withSchema(Orders::sourceSchema())
            )
            -&gt;withEntry(&#039;_address&#039;, ref(&#039;address&#039;)-&gt;unpack())
            -&gt;renameEach(rename_replace(&#039;_address.&#039;, &#039;&#039;))
            -&gt;withEntry(&#039;_item&#039;, ref(&#039;items&#039;)-&gt;expand())
            -&gt;withEntry(&#039;_item&#039;, ref(&#039;_item&#039;)-&gt;unpack())
            -&gt;renameEach(rename_replace(&#039;_item.&#039;, &#039;&#039;))
            -&gt;drop(&#039;_item&#039;, &#039;items&#039;, &#039;address&#039;)
            -&gt;withEntry(&#039;currency&#039;, lit(&#039;USD&#039;))
            -&gt;withEntry(&#039;price&#039;, ref(&#039;price&#039;)-&gt;multiply(100))
            -&gt;constrain(constraint_unique(&#039;item_id&#039;))
            -&gt;match(Orders::destinationSchema())
            -&gt;write(
                to_dbal_table_insert(
                    $this-&gt;analytical,
                    SchemaProvider::ORDER_LINE_ITEMS,
                    SqliteInsertOptions::fromArray([
                        &#039;conflict_columns&#039; =&gt; [&#039;item_id&#039;],
                    ])
                )
            )
            -&gt;run(function (Rows $rows) use ($io) {
                $io-&gt;progressAdvance($rows-&gt;count());
            }, analyze: analyze())
        ;

        $io-&gt;progressFinish();

        $io-&gt;newLine();

        $io-&gt;definitionList(
            &#039;Orders Import Summary&#039;,
            new TableSeparator(),
            [&#039;Execution time &#039; =&gt; \number_format($report-&gt;statistics()-&gt;executionTime-&gt;highResolutionTime-&gt;seconds) . &#039; seconds&#039;],
            [&#039;Memory usage &#039; =&gt; \number_format($report-&gt;statistics()-&gt;memory-&gt;max()-&gt;inMb()) . &#039; MB&#039;],
            [&#039;Rows inserted &#039; =&gt; \number_format($report-&gt;statistics()-&gt;totalRows())],
        );

        return Command::SUCCESS;
    }
}
</code></pre>

    <blockquote>
        NatÃ¼rlich ist das nicht die schÃ¶nste oder auch nur korrekteste Form. Normalerweise wÃ¼rde ein CLI-Befehl nicht die
        Definition der <code>ETL-Pipeline</code> enthalten, aber fÃ¼r die Zwecke des Beispiels ist das ein guter Start.
    </blockquote>

    <p>
        Ein dediziertes zentrales Data Warehouse ist zweifellos eine verlockende Option, besonders an Orten,
        wo mangelnde Sichtbarkeit eine effiziente Entscheidungsfindung verhindert.
    </p>
    <p>
        GlÃ¼cklicherweise ist das die Art von FunktionalitÃ¤t, die im Grunde in jedem Stadium des Projektlebens hinzugefÃ¼gt werden kann.
    </p>
    <p>
        Es mag die EinfÃ¼hrung zusÃ¤tzlicher Prozesse und eine gewisse Disziplin von den Teams erfordern, aber die Vorteile einer solchen LÃ¶sung sind enorm.
    </p>
    <ul>
        <li>Keine Sorge, dass Analytics die Systemfunktion beeintrÃ¤chtigt</li>
        <li>Wir haben Zugriff auf alle Ecken unseres Systems, jeden Microservice oder jedes Modul</li>
        <li>Eine solche zentrale Datenbank ist das beste Geschenk fÃ¼r Analysten</li>
        <li>Data Science besteht nicht mehr darin, Zeit mit Datenreinigung zu verbrennen</li>
        <li>Wir kÃ¶nnen einfach und sicher praktisch jedes Business Intelligence Tool anschlieÃŸen</li>
        <li>Wir schaffen eine Datenarbeitskultur in unserer Organisation</li>
    </ul>
    <p>
        NatÃ¼rlich kÃ¶nnen solche Ã„nderungen, wie alles Neue, schwer einzufÃ¼hren erscheinen.
        Mangelnde Erfahrung in der Datenarbeit zumindest in den Anfangsphasen lÃ¤sst diese Aufgabe
        geradezu unausfÃ¼hrbar erscheinen.
    </p>
    <p>
        Aus meiner Erfahrung geht jedoch hervor, dass es am schwierigsten ist anzufangen, wenn wir bereits haben:
    </p>
    <ul>
        <li>Einige erste datenverarbeitende <code>Pipelines</code></li>
        <li>Einige oder ein Dutzend unserer Datenschemata</li>
        <li>Komplexere Transformationen</li>
        <li>Vorbereitete Tests</li>
        <li>Etablierte Prozesse und Verfahren</li>
    </ul>
    <p>
        Die Arbeit lÃ¤uft praktisch maschinell.
    </p>
    <p>
        Es ist jedoch wichtig zu bedenken, dass es keine universelle LÃ¶sung gibt, die fÃ¼r jedes System passt.
        In jedem Fall muss der Ansatz an die Spezifika des jeweiligen Systems und der Organisation angepasst werden.
    </p>

    <h2>Wie anfangen?</h2>
    <p>
        Wenn Sie Hilfe beim Aufbau eines zentralen Data Warehouse benÃ¶tigen, helfe ich Ihnen gerne.<br/>
        <a href="https://norbert.wip/consulting">Kontaktieren Sie mich</a>, und wir erstellen gemeinsam eine LÃ¶sung, die perfekt auf Ihre BedÃ¼rfnisse zugeschnitten ist.
    </p>
    <p>
        Ich ermutige Sie auch, den <a href="https://discord.gg/5dNXfQyACW" target="_blank">Discord - Flow PHP</a> Server zu besuchen, wo
        wir direkt sprechen kÃ¶nnen.
    </p>
    <div class="img-wide">
        <img src="https://norbert.wip/assets/images/blog/analytics-in-transactional-distributed-systems/consulting_01-fa277dfb3736a033cbfcf1ac931afb08.jpg" alt="Beratung" />
    </div>

    </article>
    <div class="mb-2 mx-auto max-w-screen-lg text-center">
        <script src="https://giscus.app/client.js"
                data-repo="norberttech/norbert.tech"
                data-repo-id="MDEwOlJlcG9zaXRvcnkyMjQ0MDQwNDA="
                data-category="Comments"
                data-category-id="DIC_kwDODWAiSM4CionD"
                data-mapping="pathname"
                data-strict="0"
                data-reactions-enabled="0"
                data-emit-metadata="0"
                data-input-position="bottom"
                data-theme="light"
                data-lang="en"
                crossorigin="anonymous"
                async>
        </script>
    </div>
    </main>

    <footer class="p-4 bg-sky-50 absolute bottom-0 w-full">
        <div class="mx-auto max-w-screen-2xl text-center">
            <a href="/">by @norbert_tech</a>
        </div>
    </footer>
</body>
</html>