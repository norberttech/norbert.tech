<!doctype html>
<html lang="de">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <title>Datenanalyse in verteilten Transaktionssystemen</title>
    <meta property="og:title" content="Datenanalyse in verteilten Transaktionssystemen" />
    <meta name="twitter:title" content="Datenanalyse in verteilten Transaktionssystemen" >

    <meta name="description" content="Bricht Ihr Transaktionssystem unter der Last von Berichten zusammen? Suchen Sie nach einer M√∂glichkeit, eine einheitliche Datenquelle f√ºr Ihr verteiltes System zu schaffen? Erfahren Sie, wie Sie ein effizientes analytisches Data Warehouse aufbauen und h√§ufige Fallstricke vermeiden.">
    <meta property="og:description" content="Bricht Ihr Transaktionssystem unter der Last von Berichten zusammen? Suchen Sie nach einer M√∂glichkeit, eine einheitliche Datenquelle f√ºr Ihr verteiltes System zu schaffen? Erfahren Sie, wie Sie ein effizientes analytisches Data Warehouse aufbauen und h√§ufige Fallstricke vermeiden.">
    <meta name="twitter:description" content="Bricht Ihr Transaktionssystem unter der Last von Berichten zusammen? Suchen Sie nach einer M√∂glichkeit, eine einheitliche Datenquelle f√ºr Ihr verteiltes System zu schaffen? Erfahren Sie, wie Sie ein effizientes analytisches Data Warehouse aufbauen und h√§ufige Fallstricke vermeiden.">

    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://norbert.wip/blog/2025-08-12/de/datenanalyse-in-verteilten-transaktionssystemen" />
    <meta property="og:image" content="https://norbert.wip/assets/images/avatar-8f3c52c37f20d07c5e1631e1512bdeca.jpeg" />
    <meta property="og:image:type" content="image/svg+xml" />
    <meta property="og:image:alt" content="Norbert Orzechowicz - Personal Website" />

    <meta name="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="https://norbert.wip/blog/2025-08-12/de/datenanalyse-in-verteilten-transaktionssystemen" />
    <meta name="twitter:image" content="https://norbert.wip/assets/images/avatar-8f3c52c37f20d07c5e1631e1512bdeca.jpeg">
    <meta name="twitter:site" content="@norbert_tech" />
    <meta name="twitter:creator" content="@norbert_tech" />

    <link rel="apple-touch-icon" sizes="180x180" href="https://norbert.wip/assets/images/favicons/apple-touch-icon-9cae7ee880b4fe0bd755d300e1bca71e.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://norbert.wip/assets/images/favicons/favicon-32x32-b7a4ad4b584ab95534144e071f0e8587.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://norbert.wip/assets/images/favicons/favicon-16x16-154ca21abc06ae116c8d7ffc5713c000.png">
    <link rel="shortcut icon" href="https://norbert.wip/assets/images/favicons/favicon-db409885df78dea389e6d0b036da382c.ico">

            <style>
            @import url('https://fonts.googleapis.com/css2?family=Cabin:ital,wght@0,400..700;1,400..700&display=swap');
        </style>
        <link rel="stylesheet" href="https://norbert.wip/assets/styles/app-61740c0110d91265dcc865cf927f89cb.css">
    
            
<script type="importmap">
{
    "imports": {
        "app": "https://norbert.wip/assets/app-930adf3462cf9ab60908eb1b74cf7ca7.js",
        "@oddbird/popover-polyfill": "https://norbert.wip/assets/vendor/@oddbird/popover-polyfill/popover-polyfill.index-8f1adf02d2fe31ba0e133b65d1faeece.js",
        "https://norbert.wip/assets/bootstrap.js": "https://norbert.wip/assets/bootstrap-d78d7e12c819dedf89372fb4824c072d.js",
        "htmx.org": "https://norbert.wip/assets/vendor/htmx.org/htmx.org.index-9c972342e01d7cc4830a1976259158c7.js",
        "iconify-icon": "https://norbert.wip/assets/vendor/iconify-icon/iconify-icon.index-4471e3c6a0bda14753f1327106be2b63.js",
        "@symfony/stimulus-bundle": "https://norbert.wip/assets/@symfony/stimulus-bundle/loader-9311b8ea36bad0f6168e687b4d6dee73.js",
        "@hotwired/stimulus": "https://norbert.wip/assets/vendor/@hotwired/stimulus/stimulus.index-6f73d85e0fa2bcb3802562288e3f3042.js",
        "https://norbert.wip/assets/@symfony/stimulus-bundle/controllers.js": "https://norbert.wip/assets/@symfony/stimulus-bundle/controllers-30e149621baab8220e8ae00cc40a4a23.js",
        "https://norbert.wip/assets/controllers/hello_controller.js": "https://norbert.wip/assets/controllers/hello_controller-55882fcad241d2bea50276ea485583bc.js",
        "https://norbert.wip/assets/controllers/clipboard_controller.js": "https://norbert.wip/assets/controllers/clipboard_controller-6aefa8a9dec3271dae2f05b464bf9204.js",
        "https://norbert.wip/assets/controllers/syntax_highlight_controller.js": "https://norbert.wip/assets/controllers/syntax_highlight_controller-ae10e4cee8b4dedbf232536d05654062.js",
        "highlight.js/lib/core": "https://norbert.wip/assets/vendor/highlight.js/lib/core-a4e70e8d9019e717dfd018057650e566.js",
        "highlight.js/lib/languages/php": "https://norbert.wip/assets/vendor/highlight.js/lib/languages/php-3082801e8bbfa191b25e190b817c8690.js",
        "highlight.js/styles/github-dark.min.css": "data:application/javascript,document.head.appendChild%28Object.assign%28document.createElement%28%22link%22%29%2C%7Brel%3A%22stylesheet%22%2Chref%3A%22https%3A%2F%2Fnorbert.wip%2Fassets%2Fvendor%2Fhighlight.js%2Fstyles%2Fgithub-dark.min-4b46e20f66f76e35d6454ca4f09b57c3.css%22%7D%29%29",
        "@fontsource-variable/cabin/index.min.css": "data:application/javascript,document.head.appendChild%28Object.assign%28document.createElement%28%22link%22%29%2C%7Brel%3A%22stylesheet%22%2Chref%3A%22https%3A%2F%2Fnorbert.wip%2Fassets%2Fvendor%2F%40fontsource-variable%2Fcabin%2Findex.min-9c7d0ada22af1e85cec470dbedd33409.css%22%7D%29%29",
        "clipboard": "https://norbert.wip/assets/vendor/clipboard/clipboard.index-dcdefe2499a3b69abd4ab17006bfd09b.js",
        "highlight.js/lib/languages/shell": "https://norbert.wip/assets/vendor/highlight.js/lib/languages/shell-82883b411c0363fd67d4b2a65c9191cd.js",
        "highlight.js/lib/languages/json": "https://norbert.wip/assets/vendor/highlight.js/lib/languages/json-9d4f6cc2ef6373562a0ebc9b0e6fe3c3.js",
        "highlight.js/lib/languages/twig": "https://norbert.wip/assets/vendor/highlight.js/lib/languages/twig-68d52b8a7230c15fca667a4297a37ca5.js",
        "highlight.js/lib/languages/sql": "https://norbert.wip/assets/vendor/highlight.js/lib/languages/sql-ba78665514b9cfeb1e8435f38f0e8ab5.js",
        "highlight.js/lib/languages/javascript": "https://norbert.wip/assets/vendor/highlight.js/lib/languages/javascript-ad36fcdcd285bf2c1bee1fef5dbc226f.js",
        "highlight.js/lib/languages/xml": "https://norbert.wip/assets/vendor/highlight.js/lib/languages/xml-283bc16436bf56d2e1178a8f0b733523.js"
    }
}
</script>
<!-- ES Module Shims: Import maps polyfill for modules browsers without import maps support -->
<script async src="https://ga.jspm.io/npm:es-module-shims@1.10.0/dist/es-module-shims.js"></script>
<link rel="modulepreload" href="https://norbert.wip/assets/app-930adf3462cf9ab60908eb1b74cf7ca7.js">
<link rel="modulepreload" href="https://norbert.wip/assets/vendor/@oddbird/popover-polyfill/popover-polyfill.index-8f1adf02d2fe31ba0e133b65d1faeece.js">
<link rel="modulepreload" href="https://norbert.wip/assets/bootstrap-d78d7e12c819dedf89372fb4824c072d.js">
<link rel="modulepreload" href="https://norbert.wip/assets/vendor/htmx.org/htmx.org.index-9c972342e01d7cc4830a1976259158c7.js">
<link rel="modulepreload" href="https://norbert.wip/assets/vendor/iconify-icon/iconify-icon.index-4471e3c6a0bda14753f1327106be2b63.js">
<link rel="modulepreload" href="https://norbert.wip/assets/@symfony/stimulus-bundle/loader-9311b8ea36bad0f6168e687b4d6dee73.js">
<link rel="modulepreload" href="https://norbert.wip/assets/vendor/@hotwired/stimulus/stimulus.index-6f73d85e0fa2bcb3802562288e3f3042.js">
<link rel="modulepreload" href="https://norbert.wip/assets/@symfony/stimulus-bundle/controllers-30e149621baab8220e8ae00cc40a4a23.js">
<link rel="modulepreload" href="https://norbert.wip/assets/controllers/hello_controller-55882fcad241d2bea50276ea485583bc.js">
<script type="module">import 'app';</script>
    </head>
<body class="scroll-smooth text-black relative min-h-screen pb-16">
    <div class="sticky top-0 max-h-screen overflow-y-auto bg-white py-2 px-2 border-b border-gray-500 z-[9999] print:hidden">
        <div class="grid grid-cols-2 sm:mx-auto sm:max-w-screen-2xl md:px-4">
            <div class="text-left">
                <a href="/" class="text-lg">
                    norbert.tech
                </a>
            </div>
            <div class="text-right">
                <a href="/consulting" class="text-lg inline-flex items-center space-x-1 md:mr-4 mr-2">
                    <iconify-icon icon="lineicons:consulting" class="mr-1"></iconify-icon> Consulting
                </a>
                <a href="/blog" class="text-lg inline-flex items-center space-x-1">
                    <iconify-icon icon="ooui:articles-ltr" class="mr-1"></iconify-icon> Blog
                </a>
            </div>
        </div>
    </div>
    
    <main class="mx-auto max-w-screen-2xl mb-4 md:pt-4 px-4 lg:px-0">
            <div class="mx-auto max-w-screen-lg px-2">
        <ul class="mt-2 pl-[20px] flex gap-4">
            <li>
                <a href="/blog" class="text-blue-500 hover:underline">Go Back</a>
            </li>
                                                                <li>
                        <a href="/blog/2025-08-12/pl/analiza-danych-w-rozproszonych-systemach-transakcyjnych"
                           class="text-lg hover:opacity-80"
                           title="Polish">üáµüá± Polish</a>
                    </li>
                                                                                <li>
                        <a href="/blog/2025-08-12/en/data-analytics-in-distributed-transactional-systems"
                           class="text-lg hover:opacity-80"
                           title="English">üá∫üá∏ English</a>
                    </li>
                                                                                <li>
                        <a href="/blog/2025-08-12/fr/analyse-donnees-systemes-transactionnels-distribues"
                           class="text-lg hover:opacity-80"
                           title="French">üá´üá∑ French</a>
                    </li>
                                                                                <li>
                        <a href="/blog/2025-08-12/es/analisis-datos-sistemas-transaccionales-distribuidos"
                           class="text-lg hover:opacity-80"
                           title="Spanish">üá™üá∏ Spanish</a>
                    </li>
                                    </ul>

                    <div class="mt-4 p-4 bg-yellow-50 border border-yellow-200 rounded-lg">
                <div class="flex items-start">
                    <div class="flex-shrink-0">
                        <svg class="h-5 w-5 text-yellow-400" viewBox="0 0 20 20" fill="currentColor">
                            <path fill-rule="evenodd" d="M8.257 3.099c.765-1.36 2.722-1.36 3.486 0l5.58 9.92c.75 1.334-.213 2.98-1.742 2.98H4.42c-1.53 0-2.493-1.646-1.743-2.98l5.58-9.92zM11 13a1 1 0 11-2 0 1 1 0 012 0zm-1-8a1 1 0 00-1 1v3a1 1 0 002 0V6a1 1 0 00-1-1z" clip-rule="evenodd"/>
                        </svg>
                    </div>
                    <div class="ml-3">
                        <h3 class="font-medium text-yellow-800">
                            Translation Notice
                        </h3>
                        <div class="mt-2 text-yellow-700">
                            <p>
                                This is an automatically translated version of that Article. Despite my best efforts, it might not be perfect.<br/>
                                Native speakers are welcome to
                                <a href="https://github.com/norberttech/norbert.tech/edit/main/templates/blog/posts/2025-08-12/datenanalyse-in-verteilten-transaktionssystemen/post.html.twig"
                                   class="underline hover:text-yellow-800" target="_blank" rel="noopener">open pull requests
                                </a> to correct anything that doesn't sound right.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
            </div>
    <article class="blog-post px-2 py-5 sm:px-4 mx-auto max-w-screen-lg">
            <div class="img-wide">
        <img src="https://norbert.wip/assets/images/blog/analytics-in-transactional-distributed-systems/analytics_01-b27f8a842f645a9c7d9901f3fd11fde8.jpg" alt="Datenanalyse in verteilten Transaktionssystemen" />
    </div>

    <h1 class="font-bold text-4xl mb-2" id="title">Datenanalyse in verteilten Transaktionssystemen</h1>
    <div class="mb-2">
        <small class="text-sm">Ver√∂ffentlicht am August 12, 2025 00:00</small>
    </div>
    <div class="mb-4">
                    <small><span class="badge badge-info">Datenanalyse</span></small>
                    <small><span class="badge badge-info">Data Warehouse</span></small>
                    <small><span class="badge badge-info">ETL</span></small>
                    <small><span class="badge badge-info">Datenverarbeitung</span></small>
                    <small><span class="badge badge-info">Transaktionssysteme</span></small>
            </div>
    <p>
        In diesem Artikel m√∂chte ich das Problem der Datenanalyse in verteilten Transaktionssystemen angehen.<br/>
        Wenn Sie nach Ideen f√ºr den Aufbau eines zentralen Data Warehouse suchen, das Ihnen erm√∂glicht, Daten aus dem gesamten System zu sammeln,
        unabh√§ngig von seiner Fragmentierung und ohne in den Betriebskosten zu ertrinken, dann ist dieser Artikel f√ºr Sie.
    </p>

    <h2>Alles beginnt unschuldig</h2>
    <p>
        Die meisten Systeme, die wir t√§glich erstellen, speichern Daten in irgendeiner relationalen Datenbank.
        Eine sehr beliebte und gleichzeitig gute Wahl ist PostgreSQL, das in den letzten Jahren zu einem nahezu
        Standard in der Branche geworden ist.
    </p>
    <p>
        Die Geschichte der meisten Projekte verl√§uft meist sehr √§hnlich: Wir beginnen mit der Verifizierung der Idee, gewinnen
        unsere ersten Benutzer, das System beginnt Geld zu verdienen, das Business √ºberlegt, wie man die Gewinne steigern kann,
        neue Funktionalit√§ten entstehen. Jede neue Funktionalit√§t bedeutet einige neue Tabellen in der Datenbank.
    </p>
    <p>
        Um die Entwicklung zu beschleunigen, verwenden wir ein ORM, generieren automatisch Migrationen, die das
        Datenbankschema erstellen und aktualisieren.
    </p>
    <p>
        Anfangs l√§uft alles glatt, neue Funktionalit√§ten bringen die erwarteten Gewinne, das Business beginnt zu skalieren.
        Wir stellen mehr Programmierer ein, um mehr Funktionalit√§ten parallel zu entwickeln.
    </p>
    <p>
        Von Zeit zu Zeit meldet jemand, dass das System an manchen Stellen anf√§ngt zu "h√§ngen", schnelle Aufkl√§rung, noch
        schnellere Diagnose, es fehlt ein Index in irgendeiner Tabelle.
    </p>
    <p>
        In der ORM-Mapping-Konfiguration f√ºgen wir einen Index f√ºr das Feld hinzu, nach dem das System sehr h√§ufig Daten sucht.
        Problem gel√∂st.
    </p>
    <p>
        Das wachsende Entwicklerteam legt gro√üen Wert auf Qualit√§t, vielleicht verwendet es sogar
        fortgeschrittene Softwareentwicklungstechniken wie Event Storming oder Domain-Driven Design.<br/>
        CI/CD f√ºhrt unz√§hlige Tests aus und stellt sicher, dass √Ñnderungen keine Regressionen einf√ºhren.
    </p>
    <p>
        Die Idylle dauert an, das Team oder vielleicht mehrere Teams beginnen, neue Module zum System hinzuzuf√ºgen. Module, die angemessen
        isoliert sind, f√ºr spezifische Aufgaben verantwortlich, niemals ihre Grenzen √ºberschreiten und nicht in die
        Kompetenzen anderer Module eingreifen.
    </p>
    <p>
        F√ºr die Kommunikation werden nat√ºrlich Warteschlangen verwendet, wir implementieren das
        <a href="https://event-driven.io/en/outbox_inbox_patterns_and_delivery_guarantees_explained/" target="_blank">Outbox/Inbox
            Pattern</a>
    </p>
    <p>
        Um die entsprechende Isolation zu gew√§hrleisten, stellen wir Regeln auf, die besagen, dass jedes Modul nur
        Zugriff auf die Tabellen in der Datenbank hat, die zu ihm geh√∂ren. Um Daten aus einem anderen Modul zu erhalten, muss man
        sich an dieses Modul wenden, sei es √ºber eine interne API oder auf andere Weise.
    </p>
    <p>
        Gelegentlich kommt das Business zu uns mit der Frage: <strong>K√∂nnt ihr uns schnell diesen
            Bericht erstellen?</strong>
        Nat√ºrlich, ein paar Zeilen SQL, vielleicht ein paar Dutzend und der Bericht ist fertig.
    </p>
    <p>
        Das Business ist zufrieden, der Bericht als CSV geht zu Excel (dem beliebtesten BI-Tool), das Business zieht
        Schl√ºsse, plant neue Funktionalit√§ten und √Ñnderungen.
    </p>
    <div class="img-wide">
        <img src="https://norbert.wip/assets/images/blog/analytics-in-transactional-distributed-systems/happy_business_01-4fb1ffd438d7889256901278c0d60981.jpg" alt="Business Intelligence" />
    </div>

    <h2>Zeit vergeht, neue Tabellen wachsen wie Pilze nach dem Regen</h2>
    <p>
        In diesem Zustand k√∂nnen wir sehr lange verharren, sogar mehrere gute Jahre.
    </p>
    <p>
        In der Zwischenzeit wird jemand irgendwo sicherlich auf die Idee kommen, dem System die M√∂glichkeit zur Berichtserstellung hinzuzuf√ºgen.
        Es ist nur eine Frage der Zeit.
    </p>
    <p>
        Berichte √ºber den Systemzustand sind f√ºr das Business eines der wichtigsten Werkzeuge, die Einblick in Verhaltensweisen,
        Pr√§ferenzen oder Trends der Benutzer geben. Sie erm√∂glichen es nicht nur zu verstehen, was passiert, sondern auch angemessen zu planen, was
        erst passieren soll.
    </p>
    <p>
        Je besser und detaillierter die Berichte, desto bessere Entscheidungen k√∂nnen auf ihrer Grundlage getroffen werden. Gute
        Gesch√§ftsentscheidungen f√ºhren zu h√∂heren Gewinnen, h√∂here Gewinne f√ºhren zu einem gr√∂√üeren Budget.
        Ein gr√∂√üeres Budget f√ºhrt zu besseren Tools, gr√∂√üeren Teams, besseren Geh√§ltern oder Boni.
    </p>
    <p>
        Im Interesse jedes Programmierers sollte es daher sein, dem Business m√∂glichst gute und
        pr√§zise Daten zu liefern, schlie√ülich f√ºhren bessere Ergebnisse direkt zu besseren Gewinnen.
    </p>

    <h2>Erste Symptome</h2>
    <p>
        Das System funktioniert, bringt Gewinne. Es besteht aus etwa 5, vielleicht sogar 10 Modulen, jedes Modul besteht aus 20-50
        Tabellen in der Datenbank. Jedes Modul liefert seine eigenen Berichte.
    </p>
    <ul>
        <li>Verkauf</li>
        <li>Marketing</li>
        <li>Logistik</li>
        <li>Lagerbest√§nde</li>
        <li>Benutzer</li>
    </ul>
    <p>
        Jedes Modul stellt nur einen Teil der Daten zur Verf√ºgung, einen Bruchteil des gr√∂√üeren Bildes, keines gibt jedoch einen √úberblick √ºber das Ganze.
    </p>
    <p>
        Die Teams haben zwar Referenzschl√ºssel zu Daten aus anderen Modulen implementiert, es gelang sogar,
        in der Benutzeroberfl√§che einen Ort zu schaffen, von dem aus Berichte generiert werden k√∂nnen.
    </p>
    <p>
        Das ist jedoch immer noch zu wenig...
    </p>
    <p>
        Sehr schnell stellt sich heraus, dass Berichte, die in verschiedenen Modulen generiert werden, von verschiedenen Programmierern geschrieben,
        vielleicht sogar in verschiedenen Technologien, unterschiedliche Datenformate und verschiedene Namensstandards haben.
    </p>
    <p>
        Datumsbereiche werden unterschiedlich interpretiert, ein Modul ber√ºcksichtigt End- und Anfangsdaten, ein anderes schlie√üt sie aus,
        und ein anderer macht ein rechtsseitig offenes Intervall zur Erleichterung der Paginierung, weil sie zuf√§llig
        auch eine API haben und diese API denselben Code-Teil verwendet.
    </p>
    <p>
        Da jedes Modul unabh√§ngig ist, seine eigenen Grenzen und seine eigene Nomenklatur hat, stellen wir irgendwann fest,
        dass das, was wir in einem Modul irgendwie nennen, ein anderes Modul unter einem v√∂llig anderen Namen exponiert.
        Weil es im Kontext dieses Moduls Sinn macht.
    </p>
    <p>
        Mit der Zeit werden wir wahrscheinlich auch feststellen, dass jedes Team unterschiedlich seine Retention- und
        Datenspeicherrichtlinien definiert hat.
        Trotz des Besitzes von Daten der letzten 5 Jahre im Schl√ºsselmodul k√∂nnen wir nichts mit ihnen anfangen, weil Module, die
        Daten zur Anreicherung des Grundberichts lieferten, nur Daten der letzten 2 Jahre besitzen.
    </p>
    <p>
        Dies sind jedoch keine Probleme, die ein wenig Excel-Magie nicht l√∂sen k√∂nnte (vielleicht abgesehen von fehlenden
        Daten).
        Wir √§ndern die Namen dieser Spalten, entfernen jene, f√ºgen schnelle Filter hinzu und das reicht.
    </p>
    <p>
        Wir erstellen eine gro√üe Datei, in der wir ein Arbeitsblatt namens "Dashboard" haben, und alle
        anderen sind nur lesbar und versorgen das Dashboard.
    </p>
    <p>
        Vielleicht funktioniert dieser Ansatz sogar eine Weile. Vielleicht sogar l√§nger als eine Weile, aber machen wir uns keine Illusionen.
        Das alles wird schlie√ülich zusammenbrechen, und das gem√§√ü <a href="https://en.wikipedia.org/wiki/Murphy%27s_law" target="_blank">Murphys Gesetz</a>
        im schlimmstm√∂glichen Moment.
    </p>

    <h2>Was ist schlecht an Excel?</h2>
    <p>
        Nichts! Excel ist ein fantastisches Tool. Das Problem liegt nicht in Excel, sondern in seiner Verwendung.
    </p>
    <p>
        Diese ganze Magie der Datenreinigung und -aufbereitung sollte nicht in Excel stattfinden, nicht
        in gr√∂√üerem Ma√üstab. Wenn wir √ºber einen einmaligen schnellen Bericht sprechen, ist das kein Problem. Wir tun, was wir m√ºssen,
        klimpern Formeln, analysieren Daten und vergessen.
    </p>
    <p>
        Wenn dies jedoch Teil unserer t√§glichen Routine werden soll, wenn wir zyklisch durch denselben
        Prozess gehen m√ºssen, folgend den st√§ndigen √Ñnderungen und der Evolution des Systems, wird sich fr√ºher oder sp√§ter herausstellen, dass diese Arbeitsbl√§tter
        veraltet sind.
    </p>
    <p>
        Spalten existieren nicht mehr oder haben ihre Namen ge√§ndert, neue Spalten sind entstanden, das Datenformat hat sich ge√§ndert oder
        schlimmer noch, eines der Teams, das f√ºr eines der Module verantwortlich ist, hat einige Daten ohne das Bewusstsein gel√∂scht, dass sie
        von einem Gesch√§ftsbenutzer irgendwo in einem seiner Berichte verwendet wurden, den er einmal im Quartal √∂ffnet.
    </p>
    <p>
        Auf lange Sicht sind komplexere Tabellenkalkulationen, die Daten aus automatisch vom System generierten
        Berichten beziehen, die dann basierend auf impliziten Regeln zusammengef√ºgt werden, nicht wartbar.
    </p>

    <h2>Sollen wir vielleicht ein BI-Tool anschlie√üen?</h2>
    <p>
        Dachten viele Programmierer, die mehrfach mit dem Problem der Berichtserstellung konfrontiert waren.
    </p>
    <p>
        Nehmen wir zum Beispiel <a href="https://www.metabase.com/" target="_blank">Metabase</a>. Ein kostenloses Tool,
        das wir in wenigen Minuten mit Docker aufsetzen k√∂nnen.
    </p>
    <p>
        Geben Sie ihm Zugriff auf unsere Datenbank und einige oder alle Tabellen, und √ºber eine sehr benutzerfreundliche Oberfl√§che
        kann das Business sehr einfach und angenehm die kompliziertesten Berichte generieren.
    </p>
    <p>
        Berichte, die Daten aus mehreren Modulen gleichzeitig enthalten k√∂nnen!
    </p>
    <p>
        Wir k√∂nnen sogar einen Datenanalysten mit SQL-Grundkenntnissen einstellen, der alles das, was sich nicht
        anklicken l√§sst, mit entsprechend vorbereiteten Abfragen erreicht.
    </p>

    <h2>Nur, das l√∂st das Problem nicht</h2>
    <p>
        Es verschiebt es nur in der Zeit.
    </p>
    <p>
        Wenn wir genau hinschauen, was sich ge√§ndert hat, dann hat sich nur eine Sache ge√§ndert. Das Tool...
        Wir haben das Problem der Datenreinigung und -verkn√ºpfung von Excel zu Metabase verschoben.
    </p>
    <p>
        Excel ist zwar zu seiner urspr√ºnglichen Rolle zur√ºckgekehrt, wir k√∂nnen jetzt Berichte, die aus Metabase heruntergeladen wurden, in Excel werfen.
    </p>
    <p>
        Jedoch ist unsere implizite Logik zur Datenverkn√ºpfung/-reinigung von der Tabellenkalkulation zu SQL-Abfragen
        gewandert.
    </p>
    <p>
        Dar√ºber hinaus sind alle Probleme dieselben geblieben:
    </p>
    <ul>
        <li>Dateninkonsistenz</li>
        <li>Inkonsistente Namensgebung</li>
        <li>Fehlen einer einheitlichen R√ºckw√§rtskompatibilit√§tsrichtlinie</li>
        <li>Fehlen einer einheitlichen Datenaufbewahrungsrichtlinie</li>
    </ul>

    <h2>Sollen wir vielleicht Prozesse und Regeln einf√ºhren?</h2>
    <p>
        Die meisten der oben genannten Probleme lassen sich durch die Implementierung entsprechender Prozesse und Regeln l√∂sen.
    </p>
    <p>
        Wir k√∂nnen Namensstandards festlegen, die besagen, dass jede Tabelle in der Datenbank ein Modulpr√§fix im Namen enthalten muss, und
        Spalten in Kleinbuchstaben und durch Unterstriche getrennt benannt werden.
    </p>
    <p>
        Wir k√∂nnen festlegen, dass jedes Modul Daten der letzten 5 Jahre speichert (Hot Storage), alles √Ñltere wird
        archiviert. (Cold Storage)
    </p>
    <p>
        Wir k√∂nnen festlegen, dass Datumsbereiche immer als rechtsseitig offene Intervalle behandelt werden.
    </p>
    <p>
        Wir k√∂nnen festlegen, dass wir keine Spalten aus der Datenbank entfernen, oder dass wir vor dem Entfernen von etwas zuerst
        in eine √úbergangszeit eintreten, w√§hrend der wir jedem Systembenutzer zeigen,
        welche Spalten sich √§ndern werden und wie.
    </p>
    <p>
        Selbst wenn wir f√ºr die Diskussion annehmen, dass es gelingt, diese Prozesse global zwischen mehreren
        Teams zu implementieren und dass diese Teams sie bedingungslos und sehr genau befolgen werden, <strong>ist das nicht genug...</strong>
    </p>

    <h2>Datenbankskalierung ist nicht billig</h2>
    <p>
        Besonders wenn wir uns auf Cloud-L√∂sungen st√ºtzen.
    </p>
    <p>
        Stellen wir uns eine Situation vor, in der w√§hrend der Spitzenarbeitszeiten des Systems (wenn Benutzer die meisten
        Transaktionen generieren) ein Gesch√§ftsanalyst, der nach seinem eigenen Plan arbeitet, einen Bericht basierend auf einem typischen SQL-
        Mehrkilozeiler generieren muss?
    </p>
    <p>
        Der Analyst startet die Abfrage, die Datenbank beginnt zu mahlen. Die Abfrage dauert 5, 10, 15 Minuten.
        Die Datenbank beginnt zu schwitzen.
    </p>
    <p>
        Benutzer bombardieren das System mit neuen Bestellungen (oder anderen Operationen, die viele
        Schreibvorg√§nge generieren), w√§hrend der Analyst auf Ergebnisse wartet.
    </p>
    <p>
        Gleichzeitig muss jemand aus dem Business schnell ein paar Berichte √ºberpr√ºfen, jeder enth√§lt
        "die Gesamtzahl der Zeilen in der Tabelle".
        Es gibt mehrere solcher Personen.
    </p>
    <p>
        All diese Operationen √ºberlagern sich, unsere bereits sehr belastete Datenbank schafft es nicht.
    </p>
    <p>
        Einige Benutzertransaktionen kommen nicht durch. <br/>
        Das System atmet kaum. Die Wartezeit f√ºr grundlegendste Operationen wird in Sekunden gemessen.
    </p>
    <p>
        Und jetzt das Sahneh√§ubchen: Wenn all diese Danteske Szenen stattfinden, wenn Pager Duty gl√ºhend hei√ü ist
        von allen Arten von Vorf√§llen, wenn Teams in Panik versuchen, das System wieder zum Leben zu erwecken,
        DevOps-Leute √ºberlegen, wie man die Datenbank schnell skaliert...
    </p>
    <div class="img-wide">
        <img class="mt-[-150px]" src="https://norbert.wip/assets/images/blog/analytics-in-transactional-distributed-systems/construction_01-59e51f16aa79ecadc241f490217f29a0.jpg" alt="Renovierungsarbeiten" />
    </div>
    <p>
        Der CEO beginnt eine Pr√§sentation f√ºr einen potenziellen Gesch√§ftspartner, mit dem die Zusammenarbeit
        sich als entscheidend f√ºr die Unternehmensstrategie erweisen soll...
    </p>

    <h2>Sollen wir einfach ein Replikat aufsetzen?</h2>
    <p>
        Schlie√ülich werden Berichte unsere Transaktionsdatenbank nicht √ºberlasten.
    </p>
    <p>
        Wir verdoppeln zwar die Datenbankunterhaltungskosten, reduzieren aber das System√ºberlastungsrisiko und k√∂nnen
        unser liebstes Business Intelligence Tool direkt an das Replikat anschlie√üen, was uns Echtzeitdaten gibt.
    </p>
    <p>
        Klingt fantastisch, aber in der Praxis ist es nicht so einfach.
    </p>
    <p>
        Abgesehen von potenziellen Problemen, die sich aus der Natur der Replikation selbst ergeben, ist das Haupt- und Grundproblem,
        auf das ich am h√§ufigsten sto√üe, die <strong>Wahrnehmung</strong>.
    </p>
    <p>
        V√∂llig anders werden ein Programmierer, der diese Tabellen mit ORM-Mappings generiert hat, und ein Datenanalyst
        auf Tabellen in der Datenbank blicken.
    </p>
    <p>
        Der Programmierer wird wissen, welche Tabellen miteinander verbunden werden m√ºssen, um ein Gesamtbild zu erhalten.
        Er wird die Einschr√§nkungen und Bedingungen verstehen, die irgendwo im Anwendungscode vergraben sind.
        Vor allem kennt der Programmierer den Lebenszyklus des Systems (seiner Daten) oder sollte sich zumindest orientieren.
    </p>
    <p>
        All dieses Wissen ist f√ºr Analysten meist nicht verf√ºgbar.
    </p>
    <p>
        Es ist, als w√ºrde man jemandem sagen, er solle durch ein Schl√ºsselloch schauen. Etwas kann man sicherlich sehen.
        Einige Schl√ºsse lassen sich ziehen, aber es wird sehr schwer sein, das Ganze zu rekonstruieren.
    </p>
    <p>
        Es reicht, dass wir eine JSONB-Spalte in der Datenbank haben, in der wir einige Datenstrukturen speichern.
        Nehmen wir an, das System erlaubt 3 g√ºltige Kombinationen derselben Struktur, aber eine ist superselten, so
        selten, dass sie im System noch nicht aufgetreten ist. Beim Betrachten der Daten, auch ganzheitlich, kann der Analyst einfach nicht wissen,
        dass 3 Kombinationen einer Struktur existieren. Bei der Normalisierung wird er 2 F√§lle ber√ºcksichtigen, w√§hrend der dritte
        zu einer tickenden Zeitbombe wird, die wie immer im unerwartesten Moment explodiert.
    </p>
    <p>
        Anders gesagt, wenn wir mehrere unabh√§ngige Module im System haben. Jedes mit seiner eigenen Datenbank oder zumindest
        seinen eigenen Tabellen in der Datenbank. Was zusammen 200-300 Tabellen ergibt, ist die Erwartung, dass der Analyst das ohne Probleme
        bew√§ltigt, keine Fehler macht und Berichte nicht von den Erwartungen abweichen, gelinde gesagt naiv.
    </p>
    <p>
        Trotz allem ist das Aufstellen einer Kopie/Replikat-Datenbank f√ºr Analysten und die Vergabe eines 4-buchstabigen Namens, der vom
        Wort "Analytics" stammt, immer noch weit verbreitet.
    </p>
    <p>
        BI-Tools √ºberbieten sich darin, wer die bessere Benutzeroberfl√§che erstellt, mit der sich Berichte anklicken lassen.
        Sie versprechen, dass wir Daten ohne SQL analysieren k√∂nnen.
    </p>
    <p>
        Ja, das kann funktionieren, an vielen Stellen funktioniert es genau so. Wor√ºber wir jedoch nicht laut sprechen:
    </p>
    <ul>
        <li>Probleme mit R√ºckw√§rtskompatibilit√§t und Datenstruktur√§nderungen</li>
        <li>Probleme mit der ordnungsgem√§√üen Wartung/Versionierung/Tests von gigantischen SQL-Abfragen/Skripten zur
            Datennormalisierung im laufenden Betrieb
        </li>
        <li>Replikate/Kopien erzeugen zus√§tzliche Kosten</li>
        <li>Die Reduzierung der Replikatressourcen ist entweder unm√∂glich oder macht die Berichtserstellung in akzeptablen
            Zeiten unm√∂glich
        </li>
    </ul>
    <p>
        Was sich letztendlich auf die Datenqualit√§t und die Effektivit√§t der Gesch√§ftsentscheidungen auswirkt.
    </p>

    <h2>Was bleibt uns √ºbrig?</h2>
    <p>
        Vielleicht sollten wir zuerst festlegen, welche Probleme wir in erster Linie l√∂sen wollen:
    </p>
    <div class="img-wide">
        <img src="https://norbert.wip/assets/images/blog/analytics-in-transactional-distributed-systems/strategy_01-f82682bfa13643ba5b8957e806e0d823.jpg" alt="Strategie und Analyse" />
    </div>
    <ol>
        <li>Datenanalyse/Berichtserstellung darf keinen Einfluss auf die Systemfunktion haben.</li>
        <li>Daten in Berichten m√ºssen immer aktuell sein (Datenverz√∂gerung ist akzeptabel, individuell festgelegt)</li>
        <li>Berichte m√ºssen den realen, unverzerrten Systemzustand widerspiegeln</li>
        <li>Die Datenstruktur muss regressionsresistent sein</li>
        <li>Einheitliche Datenaufbewahrungs- und Archivierungsrichtlinie</li>
    </ol>

    <h2>1) Ressourcentrennung</h2>
    <p>
        Das ist nichts Revolution√§res, wenn wir nicht wollen, dass unser System durch √úberlastung der Datenbank
        durch Berichtserstellung gef√§hrdet wird, m√ºssen wir eine separate Datenbank aufsetzen.
    </p>
    <p><strong>Welche Datenbank f√ºr Analytics w√§hlen?</strong></p>
    <p>
        Das ist im Grunde ein Thema f√ºr einen separaten Artikel oder sogar eine Artikelserie.
        Es gibt sehr viele L√∂sungen, einige bessere, andere schlechtere. Es gibt keine einzige
        magische L√∂sung f√ºr alle Probleme.
    </p>
    <p>
        Mein Rat, besonders f√ºr kleinere Teams ohne Erfahrung im Datenmanagement, ist, sich nicht
        auf Technologien zu st√ºrzen, mit denen wir keine Erfahrung haben.
    </p>
    <p>
        Entscheidend ist das richtige Datenformat. Nach der Umwandlung vieler schmaler Tabellen in eine breite wird sich wahrscheinlich
        herausstellen, dass die Generierung desselben Berichts ohne 20x <code>JOIN</code> nicht mehr 10 Minuten
        sondern weniger als eine halbe Sekunde dauert.
    </p>
    <p>
        Und wenn das Problem Aggregationen sind, nicht Verkn√ºpfungen?
    </p>
    <p>
        Dann ist es besser, anstatt im laufenden Betrieb zu aggregieren, eine Tabelle vorzubereiten, die diese Daten in aggregierter
        und nicht in roher Form enth√§lt.
    </p>

    <h2>2) Aktuelle Daten</h2>
    <p>
        Nun gut, aber wenn wir eine neue, unabh√§ngige Datenbank erstellen, wie sorgen wir daf√ºr, dass die Daten in dieser
        Datenbank aktuell und frisch sind?
    </p>
    <p>
        Hier h√§ngt sehr viel von der akzeptablen Verz√∂gerung bei der Datensynchronisation ab.
        Meist reicht es, wenn die Analysedatenbank etwa 24 Stunden hinter der Transaktionsdatenbank liegt. Das hei√üt, sie enth√§lt
        Daten bis "gestern", einschlie√ülich des ganzen "gestrigen Tages".
    </p>
    <p>
        Warum? Weil nur wenige Gesch√§ftsentscheidungen sofort getroffen werden.
        Wenn Entscheidungen in so kurzer Zeit getroffen werden m√ºssen, dann baut man entsprechende Automatisierungen.
    </p>
    <p>
        Wenn eine 24-st√ºndige Verz√∂gerung akzeptabel ist (manchmal ist sie es nicht und daf√ºr gibt es auch L√∂sungen),
        reicht es, wenn wir die Synchronisation mehrmals t√§glich durchf√ºhren.
        Nat√ºrlich gibt es auch hier keine goldene Regel. Genauso wie es keine Regel gibt, die besagt, welchen Bereich man auf einmal synchronisieren soll.
    </p>
    <p>
        Es gibt jedoch eine gute Praxis, die die Synchronisation erleichtert. Sie besteht darin, sicherzustellen, dass die Haupttabellen im
        Transaktionssystem das Datum der Erstellung/√Ñnderung des Datensatzes enthalten.
    </p>
    <p>
        Mit diesen beiden Informationen k√∂nnen wir das Synchronisationsfenster auf einen bestimmten Zeitraum eingrenzen.
    </p>
    <p>
        Wie sieht das in der Praxis aus? Wir k√∂nnen z.B. alle 6 Stunden einen Synchronisationsprozess starten und nur Datens√§tze sammeln, die in den
        letzten 24 Stunden ge√§ndert wurden.<br/>
        <code>Das sind nat√ºrlich Beispielzahlen, diese Werte m√ºssen basierend auf der Gr√∂√üe und dem Verhalten der Daten festgelegt werden.</code>
    </p>
    <p>
        Warum aus 24 Stunden? Als zus√§tzliche Sicherheit. Wir k√∂nnten Daten nur aus 7 Stunden holen, aber wenn aus irgendeinem
        Grund die Synchronisation nicht ausgef√ºhrt wird und wir das nicht bemerken, k√∂nnten wir Daten verlieren.
    </p>

    <h2>3) Systemzustand widerspiegeln</h2>
    <p>
        Meine Meinung zu diesem Thema mag kontrovers erscheinen, aber ich glaube, dass das beste Wissen √ºber Daten und Systemverhalten
        das Team hat, das dieses System/Modul baut.
    </p>
    <p>
        Genau dieses Team sollte daf√ºr verantwortlich sein, dass Daten, die vom System oder seinem Teil generiert werden,
        f√ºr den das gegebene Team verantwortlich ist, in das zentrale Datenrepository gelangen.
    </p>
    <p>
        Mit anderen Worten, genau das Team, das eine bestimmte Funktionalit√§t implementiert, sollte basierend auf zuvor gesammelten Anforderungen
        diese Daten in das entsprechende Format umwandeln und weiterleiten.
    </p>
    <p>
        Dies ist wahrscheinlich der einfachste Weg sicherzustellen, dass die Daten vollst√§ndig sind und Programmierer aus dem jeweiligen Team sich
        bewusst sind, dass diese Daten irgendwo verwendet werden. Das analytische Datenformat wird f√ºr sie zu
        einer Art Vertrag ‚Äì einem Vertrag, den sie einhalten m√ºssen.
    </p>
    <p>
        Das unterscheidet sich nicht sehr vom API-Schema-Vertrag.
    </p>

    <h2>4) Regressionsresistenz</h2>
    <p>
        Dieser Punkt ist wahrscheinlich der komplizierteste. Die korrekte Implementierung der Datenschema-Evolution ist
        oft nicht so sehr schwierig als vielmehr m√ºhsam.
    </p>
    <p>
        In Kurzform sehen die Regeln so aus:
    </p>
    <ul>
        <li>Wir entfernen niemals Spalten</li>
        <li>Alle Spalten, die wir hinzuf√ºgen, m√ºssen <code>nullable</code> sein oder einen Standardwert haben</li>
        <li>Spaltentypen k√∂nnen wir nur erweitern, zum Beispiel k√∂nnen wir <code>int</code> in <code>bigint</code> umwandeln, aber nicht umgekehrt</li>
        <li>Wir √§ndern keine Spaltennamen</li>
    </ul>
    <p>
        K√∂nnen wir also nichts l√∂schen?
    </p>
    <p>
        K√∂nnen wir, aber nicht einfach so. Generell h√§ngt es nur von uns ab, wie und wie oft wir die R√ºckw√§rtskompatibilit√§t brechen.
    </p>
    <p>
        Wenn wir unsere analytische Datenquelle nur intern nutzen und, sagen wir, der Analyst, der Berichte erstellt,
        auf dem Laufenden mit System√§nderungen ist, k√∂nnten wir bei entsprechender Koordination neue Tabellen hinzuf√ºgen
        und dann alte entfernen, ihm etwas Zeit geben, die Berichte zu aktualisieren.
    </p>
    <p>
        Wenn jedoch unsere analytische Datenquelle f√ºr <code>Data Science</code> verwendet wird, aber wir in einer
        Multi-Tenancy-Umgebung arbeiten und analytische Daten/Berichte Kunden zur Verf√ºgung gestellt werden, dann m√ºssen wir v√∂llig anders an die Sache herangehen.
    </p>

    <h2>Datenaufbewahrungs- und Archivierungsrichtlinie</h2>
    <p>
        Wie ich oben erw√§hnt habe, ist es sehr wichtig, dass Daten in der Analysedatenbank, insbesondere die von verschiedenen
        Modulen gelieferten, denselben Regeln bez√ºglich der Aufbewahrungszeit unterliegen.
    </p>
    <p>
        Wenn wir Lagerzust√§nde im System nur aus dem letzten Jahr speichern, aber Bestellungen aus den letzten 5 Jahren,
        k√∂nnen Analysten keinen Bericht erstellen, der Daten aus beiden Quellen enth√§lt.
    </p>
    <p>
        Das ist eher ein formales als ein technisches Problem. Es scheint, dass es ausreichen w√ºrde, sich einfach zu einigen,
        aber in der Praxis ist es nicht so einfach.
    </p>
    <p>
        Um eine gemeinsame Datenaufbewahrungs- und Archivierungsrichtlinie festzulegen, m√ºssen nicht nur technische,
        sondern auch rechtliche, gesch√§ftliche oder analytische Aspekte ber√ºcksichtigt werden, was Kompromisse erfordern kann.
    </p>

    <h2>Beispiele</h2>
    <p>
        Schauen wir uns nun ein einfaches Beispiel eines ETL-Prozesses an, dessen Aufgabe es ist, Daten von der Transaktionsdatenbank
        zur Analysedatenbank zu √ºbertragen.
    </p>
    <blockquote>
        In diesem Beispiel verwende ich <a href="https://flow-php.com" target="_blank">Flow PHP</a>, das ist
        jedoch nichts speziell Einzigartiges f√ºr PHP. In jeder Sprache k√∂nnen wir etwas sehr √Ñhnliches mit
        jeder Bibliothek erstellen, die die Erstellung von CLI-Anwendungen und einem Tool zur Datenverarbeitung erleichtert.
    </blockquote>
    <p>
        Das folgende Beispiel (in etwas ver√§nderter Form) stammt aus einer Live-Stream-Session, die ich das Vergn√ºgen hatte, mit Roland aufzunehmen, der den Kanal <a href="https://nevercodealone.de/de" target="_blank">Never Code Alone</a> betreibt.
        Das Videomaterial finden Sie auf YouTube unter dem Begriff "Flow PHP"
    </p>
    <p>
        Nehmen wir an, dass das Bestellformat etwa so aussieht:
    </p>
    <pre><code class="code-shell" data-controller="syntax-highlight">schema
|-- order_id: ?uuid
|-- seller_id: uuid
|-- created_at: datetime
|-- updated_at: datetime
|-- cancelled_at: ?datetime
|-- discount: ?float
|-- email: string
|-- customer: string
|-- address: map&lt;string, string&gt;
|-- notes: list&lt;string&gt;
|-- items: list&lt;structure{sku: string, quantity: integer, price: float}&gt;</code></pre>

    <p>
        Unser Ziel ist es, diese Bestellungen in die Analysedatenbank zu √ºbertragen, also bereiten wir das Schema der Eingabedaten
        sowie der Zieldaten vor.
    </p>

    <pre><code class="code-php" data-controller="syntax-highlight">&lt;?php

declare(strict_types=1);

namespace App\DataFrames;

use Flow\ETL\Adapter\Doctrine\DbalMetadata;
use function Flow\ETL\DSL\integer_schema;
use function Flow\ETL\DSL\uuid_schema;
use function Flow\ETL\DSL\datetime_schema;
use function Flow\ETL\DSL\float_schema;
use function Flow\ETL\DSL\string_schema;
use function Flow\ETL\DSL\map_schema;
use function Flow\Types\DSL\type_map;
use function Flow\Types\DSL\type_string;
use function Flow\ETL\DSL\list_schema;
use function Flow\Types\DSL\type_list;
use function Flow\Types\DSL\type_structure;
use function Flow\Types\DSL\type_integer;
use function Flow\Types\DSL\type_float;
use function \Flow\ETL\DSL\schema;
use Flow\ETL\Schema;

final class Orders
{
    public static function sourceSchema() : Schema
    {
        return schema(
            uuid_schema(&quot;order_id&quot;),
            uuid_schema(&quot;seller_id&quot;),
            datetime_schema(&quot;created_at&quot;),
            datetime_schema(&quot;updated_at&quot;, nullable: true),
            datetime_schema(&quot;cancelled_at&quot;, nullable: true),
            float_schema(&quot;discount&quot;, nullable: true),
            string_schema(&quot;email&quot;),
            string_schema(&quot;customer&quot;),
            map_schema(&quot;address&quot;, type: type_map(key_type: type_string(), value_type: type_string())),
            list_schema(&quot;notes&quot;, type: type_list(element: type_string())),
            list_schema(&quot;items&quot;, type: type_list(element: type_structure(elements: [&quot;item_id&quot; =&gt; type_string(), &quot;sku&quot; =&gt; type_string(), &quot;quantity&quot; =&gt; type_integer(), &quot;price&quot; =&gt; type_float()]))),
        );
    }

    public static function destinationSchema() : Schema
    {
        return self::sourceSchema()
            -&gt;replace(&#039;updated_at&#039;, datetime_schema(&quot;updated_at&quot;))
            -&gt;remove(&#039;address&#039;)
            -&gt;add(
                string_schema(&#039;street&#039;, metadata: DbalMetadata::length(2048)),
                string_schema(&#039;city&#039;, metadata: DbalMetadata::length(512)),
                string_schema(&#039;zip&#039;, metadata: DbalMetadata::length(32)),
                string_schema(&#039;country&#039;, metadata: DbalMetadata::length(128)),
            )
            -&gt;remove(&#039;items&#039;)
            -&gt;add(
                uuid_schema(&#039;item_id&#039;, metadata: DbalMetadata::primaryKey()),
                string_schema(&#039;sku&#039;, metadata: DbalMetadata::length(64)),
                integer_schema(&#039;quantity&#039;),
                integer_schema(&#039;price&#039;),
                string_schema(&#039;currency&#039;, metadata: DbalMetadata::length(3)),
            )
            ;
    }
}</code></pre>

    <p>
        Beachten Sie, dass die Zielstruktur der Tabelle nicht mehr auf Bestellungen ausgerichtet ist, sondern auf bestellte Artikel.
        Unser Ziel ist es, die Bestellartikel so zu entpacken, dass jeder eine separate Zeile ist.
    </p>
    <p>
        Dadurch muss der Analyst, der einen Bericht generieren muss, nicht mehr kombinieren und
        JSON im laufenden Betrieb entpacken.
    </p>
    <p>
        Die Adressspalte wurde auch in mehrere Spalten aufgeteilt, wodurch der Bericht einfacher
        gefiltert werden kann.
    </p>
    <p>
        Eine weitere wichtige Transformation ist die Umwandlung von <code>price</code> von <code>float</code> in <code>int</code>
        durch Multiplikation des Gleitkommawerts mit 100.
    </p>
    <p>
        Die letzte √Ñnderung wird das Hinzuf√ºgen von Informationen √ºber die W√§hrung der Preise sein. Aber woher kommt diese Information?
        Das ist ein sehr wichtiges Detail, das aus einer nicht sehr guten Implementierung resultiert.
        In diesem speziellen Fall sind alle Bestellungen in Dollar. Das System wei√ü das, die Programmierer wissen das,
        aber eine Person, die die Tabellen in der Datenbank ohne Kontext betrachtet, muss dieses Wissen nicht unbedingt haben.
    </p>
    <p>
        Unsere Zielstruktur sollte etwa so aussehen:
    </p>

    <pre><code class="code-shell" data-controller="syntax-highlight">schema
|-- order_id: uuid
|-- seller_id: uuid
|-- created_at: datetime
|-- updated_at: datetime
|-- cancelled_at: ?datetime
|-- discount: ?float
|-- email: string
|-- customer: string
|-- notes: list&lt;string&gt;
|-- street: string
|-- city: string
|-- zip: string
|-- country: string
|-- item_id: uuid
|-- sku: string
|-- quantity: integer
|-- price: integer
|-- currency: string</code></pre>

    <p>
        Der n√§chste Schritt ist die Erstellung der entsprechenden Tabelle in der Analysedatenbank. Das k√∂nnen wir relativ
        einfach mit dem Adapter f√ºr Doctrine DBAL erreichen.
    </p>

    <pre><code class="code-php" data-controller="syntax-highlight">&lt;?php

declare(strict_types=1);

namespace App\Dbal;

use App\DataFrames\Orders;
use App\DataFrames\OrdersCSV;
use Doctrine\DBAL\Schema\Schema;
use Doctrine\Migrations\Provider\SchemaProvider as MigrationsSchemaProvider;
use function Flow\ETL\Adapter\Doctrine\to_dbal_schema_table;

final class SchemaProvider implements MigrationsSchemaProvider
{
    public const ANALYTICAL_ORDER_LINE_ITEMS = &#039;order_line_items&#039;;

    public function createSchema(): Schema
    {
        return new Schema(
            tables: [
                to_dbal_schema_table(Orders::destinationSchema(), self::ANALYTICAL_ORDER_LINE_ITEMS),
            ]
        );
    }
}</code></pre>

    <p>
        In der Analysedatenbank werden wir also eine "vereinfachte" oder "normalisierte" Version der Bestelltabelle speichern.
        Die Normalisierung besteht im Entpacken der Bestellartikel und ihrer Umwandlung in separate Zeilen sowie
        der Aufteilung der "Adress"-Spalte in mehrere Spalten.
    </p>

    <p>
        Schauen wir uns also den CLI-Befehl an, der f√ºr die √úbertragung von Daten von der Transaktions-
        zur Analysedatenbank verantwortlich ist.
    </p>

    <pre><code class="code-php" data-controller="syntax-highlight">&lt;?php

namespace App\Command;

use App\DataFrames\Orders;
use App\DataFrames\OrdersCSV;
use App\Dbal\SchemaProvider;
use Doctrine\DBAL\Connection;
use Flow\Doctrine\Bulk\Dialect\SqliteInsertOptions;
use Flow\ETL\Rows;
use Symfony\Component\Console\Attribute\AsCommand;
use Symfony\Component\Console\Command\Command;
use Symfony\Component\Console\Helper\TableSeparator;
use Symfony\Component\Console\Input\InputInterface;
use Symfony\Component\Console\Input\InputOption;
use Symfony\Component\Console\Output\OutputInterface;
use Symfony\Component\Console\Style\SymfonyStyle;
use function Flow\ETL\Adapter\Doctrine\from_dbal_key_set_qb;
use function Flow\ETL\Adapter\Doctrine\pagination_key_desc;
use function Flow\ETL\Adapter\Doctrine\pagination_key_set;
use function Flow\ETL\Adapter\Doctrine\to_dbal_table_insert;
use function Flow\ETL\DSL\analyze;
use function Flow\ETL\DSL\constraint_unique;
use function Flow\ETL\DSL\data_frame;
use function Flow\ETL\DSL\lit;
use function Flow\ETL\DSL\ref;
use function Flow\ETL\DSL\rename_replace;
use function Flow\ETL\DSL\schema_to_ascii;
use function Flow\Types\DSL\type_datetime;

#[AsCommand(
    name: &#039;app:orders:import&#039;,
    description: &#039;Import orders from the transactional database to the analytical database.&#039;,
)]
class OrdersImportCommand extends Command
{
    public function __construct(
        private readonly Connection $transactional,
        private readonly Connection $analytical,
    )
    {
        parent::__construct();
    }

    protected function configure()
    {
        $this-&gt;addOption(&#039;start-date&#039;, null, InputOption::VALUE_REQUIRED, &#039;Start date for the data pull.&#039;, &#039;-24 hours&#039;)
            -&gt;addOption(&#039;end-date&#039;, null, InputOption::VALUE_REQUIRED, &#039;End date for the data pull.&#039;, &#039;now&#039;)
        ;
    }


    protected function execute(InputInterface $input, OutputInterface $output): int
    {
        $io = new SymfonyStyle($input, $output);

        $io-&gt;title(&#039;Importing orders&#039;);

        $startDate = type_datetime()-&gt;cast($input-&gt;getOption(&#039;start-date&#039;));
        $endDate = type_datetime()-&gt;cast($input-&gt;getOption(&#039;end-date&#039;));

        $io-&gt;progressStart();

        $report = data_frame()
            -&gt;read(
                from_dbal_key_set_qb(
                    $this-&gt;transactional,
                    $this-&gt;transactional-&gt;createQueryBuilder()
                        -&gt;select(&#039;*&#039;)
                        -&gt;from(SchemaProvider::ORDERS)
                        -&gt;where(&#039;updated_at BETWEEN :start_date AND :end_date&#039;)
                        -&gt;setParameter(&#039;start_date&#039;, $startDate-&gt;format(&#039;Y-m-d H:i:s&#039;))
                        -&gt;setParameter(&#039;end_date&#039;, $endDate-&gt;format(&#039;Y-m-d H:i:s&#039;)),
                    pagination_key_set(
                        pagination_key_desc(&#039;updated_at&#039;),
                        pagination_key_desc(&#039;order_id&#039;)
                    )
                )-&gt;withSchema(Orders::sourceSchema())
            )
            -&gt;withEntry(&#039;_address&#039;, ref(&#039;address&#039;)-&gt;unpack())
            -&gt;renameEach(rename_replace(&#039;_address.&#039;, &#039;&#039;))
            -&gt;withEntry(&#039;_item&#039;, ref(&#039;items&#039;)-&gt;expand())
            -&gt;withEntry(&#039;_item&#039;, ref(&#039;_item&#039;)-&gt;unpack())
            -&gt;renameEach(rename_replace(&#039;_item.&#039;, &#039;&#039;))
            -&gt;drop(&#039;_item&#039;, &#039;items&#039;, &#039;address&#039;)
            -&gt;withEntry(&#039;currency&#039;, lit(&#039;USD&#039;))
            -&gt;withEntry(&#039;price&#039;, ref(&#039;price&#039;)-&gt;multiply(100))
            -&gt;constrain(constraint_unique(&#039;item_id&#039;))
            -&gt;match(Orders::destinationSchema())
            -&gt;write(
                to_dbal_table_insert(
                    $this-&gt;analytical,
                    SchemaProvider::ORDER_LINE_ITEMS,
                    SqliteInsertOptions::fromArray([
                        &#039;conflict_columns&#039; =&gt; [&#039;item_id&#039;],
                    ])
                )
            )
            -&gt;run(function (Rows $rows) use ($io) {
                $io-&gt;progressAdvance($rows-&gt;count());
            }, analyze: analyze())
        ;

        $io-&gt;progressFinish();

        $io-&gt;newLine();

        $io-&gt;definitionList(
            &#039;Orders Import Summary&#039;,
            new TableSeparator(),
            [&#039;Execution time &#039; =&gt; \number_format($report-&gt;statistics()-&gt;executionTime-&gt;highResolutionTime-&gt;seconds) . &#039; seconds&#039;],
            [&#039;Memory usage &#039; =&gt; \number_format($report-&gt;statistics()-&gt;memory-&gt;max()-&gt;inMb()) . &#039; MB&#039;],
            [&#039;Rows inserted &#039; =&gt; \number_format($report-&gt;statistics()-&gt;totalRows())],
        );

        return Command::SUCCESS;
    }
}
</code></pre>

    <blockquote>
        Nat√ºrlich ist das nicht die sch√∂nste oder auch nur korrekteste Form. Normalerweise w√ºrde ein CLI-Befehl nicht die
        Definition der <code>ETL-Pipeline</code> enthalten, aber f√ºr die Zwecke des Beispiels ist das ein guter Start.
    </blockquote>

    <p>
        Ein dediziertes zentrales Data Warehouse ist zweifellos eine verlockende Option, besonders an Orten,
        wo mangelnde Sichtbarkeit eine effiziente Entscheidungsfindung verhindert.
    </p>
    <p>
        Gl√ºcklicherweise ist das die Art von Funktionalit√§t, die im Grunde in jedem Stadium des Projektlebens hinzugef√ºgt werden kann.
    </p>
    <p>
        Es mag die Einf√ºhrung zus√§tzlicher Prozesse und eine gewisse Disziplin von den Teams erfordern, aber die Vorteile einer solchen L√∂sung sind enorm.
    </p>
    <ul>
        <li>Keine Sorge, dass Analytics die Systemfunktion beeintr√§chtigt</li>
        <li>Wir haben Zugriff auf alle Ecken unseres Systems, jeden Microservice oder jedes Modul</li>
        <li>Eine solche zentrale Datenbank ist das beste Geschenk f√ºr Analysten</li>
        <li>Data Science besteht nicht mehr darin, Zeit mit Datenreinigung zu verbrennen</li>
        <li>Wir k√∂nnen einfach und sicher praktisch jedes Business Intelligence Tool anschlie√üen</li>
        <li>Wir schaffen eine Datenarbeitskultur in unserer Organisation</li>
    </ul>
    <p>
        Nat√ºrlich k√∂nnen solche √Ñnderungen, wie alles Neue, schwer einzuf√ºhren erscheinen.
        Mangelnde Erfahrung in der Datenarbeit zumindest in den Anfangsphasen l√§sst diese Aufgabe
        geradezu unausf√ºhrbar erscheinen.
    </p>
    <p>
        Aus meiner Erfahrung geht jedoch hervor, dass es am schwierigsten ist anzufangen, wenn wir bereits haben:
    </p>
    <ul>
        <li>Einige erste datenverarbeitende <code>Pipelines</code></li>
        <li>Einige oder ein Dutzend unserer Datenschemata</li>
        <li>Komplexere Transformationen</li>
        <li>Vorbereitete Tests</li>
        <li>Etablierte Prozesse und Verfahren</li>
    </ul>
    <p>
        Die Arbeit l√§uft praktisch maschinell.
    </p>
    <p>
        Es ist jedoch wichtig zu bedenken, dass es keine universelle L√∂sung gibt, die f√ºr jedes System passt.
        In jedem Fall muss der Ansatz an die Spezifika des jeweiligen Systems und der Organisation angepasst werden.
    </p>

    <h2>Wie anfangen?</h2>
    <p>
        Wenn Sie Hilfe beim Aufbau eines zentralen Data Warehouse ben√∂tigen, helfe ich Ihnen gerne.<br/>
        <a href="https://norbert.wip/consulting">Kontaktieren Sie mich</a>, und wir erstellen gemeinsam eine L√∂sung, die perfekt auf Ihre Bed√ºrfnisse zugeschnitten ist.
    </p>
    <p>
        Ich ermutige Sie auch, den <a href="https://discord.gg/5dNXfQyACW" target="_blank">Discord - Flow PHP</a> Server zu besuchen, wo
        wir direkt sprechen k√∂nnen.
    </p>
    <div class="img-wide">
        <img src="https://norbert.wip/assets/images/blog/analytics-in-transactional-distributed-systems/consulting_01-fa277dfb3736a033cbfcf1ac931afb08.jpg" alt="Beratung" />
    </div>

    </article>
    <div class="mb-2 mx-auto max-w-screen-lg text-center">
        <script src="https://giscus.app/client.js"
                data-repo="norberttech/norbert.tech"
                data-repo-id="MDEwOlJlcG9zaXRvcnkyMjQ0MDQwNDA="
                data-category="Comments"
                data-category-id="DIC_kwDODWAiSM4CionD"
                data-mapping="pathname"
                data-strict="0"
                data-reactions-enabled="0"
                data-emit-metadata="0"
                data-input-position="bottom"
                data-theme="light"
                data-lang="en"
                crossorigin="anonymous"
                async>
        </script>
    </div>
    </main>

    <footer class="p-4 bg-sky-50 absolute bottom-0 w-full">
        <div class="mx-auto max-w-screen-2xl text-center">
            <a href="/">by @norbert_tech</a>
        </div>
    </footer>
</body>
</html>